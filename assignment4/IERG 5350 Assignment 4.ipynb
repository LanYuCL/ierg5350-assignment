{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1XwPRD9OLu"
   },
   "source": [
    "# IERG 5350 Assignment 4: Advanced Algorithms for Continuous Control in RL\n",
    "\n",
    "### Welcome to assignment 4 of our RL course!\n",
    "*2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n",
    "\n",
    "\n",
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| Ruikang Zhang | 1155150526 |\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "In this assignment, we will implement a system of RL that allows us to train and evaluate RL agents formally and efficiently.\n",
    "\n",
    "In this notebook, you will go through the following components of the whole system:\n",
    "- Preparation: Colab, and Environment\n",
    "- Section 1: Training with algorithm PPO\n",
    "- Section 2: Training with algorithm DDPG\n",
    "- Section 3: Training with algorithm TD3\n",
    "- Section 4: Transfer your PPO/ DDPG/ TD3 to another task: Four-Solution-Maze\n",
    "\n",
    "The author of this assignment is SUN, Hao (sh018 AT ie.cuhk.edu.hk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyLEr-yOpxvD"
   },
   "source": [
    "# Colab\n",
    "\n",
    "### Introduction to Google Colab: \n",
    "From now on, our assignment as well as the final project will be based on the Google Colab, where you can apply for free GPU resources to accelerate the learning of your RL models. \n",
    "\n",
    "Here are some resources as intro to the Colab.\n",
    "\n",
    "- YouTube Video: https://www.youtube.com/watch?v=inN8seMm7UI\n",
    "- Colab Intro: https://colab.research.google.com/notebooks/intro.ipynb\n",
    "(you may need to login with your google account)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gym Continuous Control Tasks\n",
    "\n",
    "### Introduction to the Gym Continuous Control Envirionments\n",
    "\n",
    "In the last assignment, you have already used the gym[atari] benchmarks, where the action space is discrete so that normal approach is value-based methods e.g., DQN.\n",
    "\n",
    "In this assignment, we will try to implement three prevailing RL algorithms for continuous control tasks, namely the PPO(https://arxiv.org/abs/1707.06347), DDPG(https://arxiv.org/abs/1509.02971) and TD3(https://arxiv.org/abs/1802.09477).\n",
    "\n",
    "We will now begin with a gym environment for continuous control,\n",
    "\n",
    "The Pendulum-v0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "2CjdPG_oqT1l",
    "outputId": "541724ca-af57-4bed-817f-728b21e8fb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state space is like [ 0.59768775 -0.80172898  0.48203772]\n",
      "the max and min action is:  [2.] [-2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'so that you may need to use action value re-size if you want to use the tanh activation functions'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "ENV_NAME = \"Pendulum-v0\"\n",
    "env = gym.make(ENV_NAME)\n",
    "state = env.reset()\n",
    "print('the state space is like', state)\n",
    "print('the max and min action is: ',env.action_space.high,env.action_space.low)\n",
    "\n",
    "'''so that you may need to use action value re-size if you want to use the tanh activation functions'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7IHoYJWur1S"
   },
   "source": [
    "# PPO \n",
    "\n",
    "The Proximal Policy Optimization Algorithms is the most prevailing on-policy learning method. Although its sample efficiency is not as high as the off-policy methods, the PPO is relatively easy to implement and the learning is much more stable than off-policy methods. Whenever you have a task you want to try whether RL works, you may try to run a PPO agent at first. It is worth mentioning even the most challenging game, the StarCraftII agent AlphaStar is trained based on PPO (with lots of improvements, ofcourse).\n",
    "\n",
    "\n",
    "## TODOs for You\n",
    "The ppo has the benfitsof trust region policy optimization (TRPO) but is much simpler to implement, and with some implementation engeneering, the sample complexity of TRPO is further improved.\n",
    "\n",
    "The key idea of PPO optimization is *Not Optimize the Policy Too Much in a Certain Step*, which follows the key insight of the method of TRPO.\n",
    "\n",
    "In TRPO, the optimization objective of policy is to learn a policy such that \n",
    "\n",
    "$$\\max_\\theta \\hat{\\mathbb{E}}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t]$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(\\cdot|s_t),\\pi_\\theta(\\cdot|s_t)]] \\le \\delta$$\n",
    "\n",
    "where $\\hat{A}$ denotes the advantage function, rather than optimize the objective function of \n",
    "\n",
    "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}_t[\\log \\pi_\\theta(a_t|s_t)\\hat{A}_t]$$\n",
    "\n",
    "in the normal policy gradint methods.\n",
    "\n",
    "The PPO proposed two alternative approaches to solve the constrained optimization above, namely the Clipped Surrogated Objective and the Adaptive KL penalty Coefficient. The former one is more generally used in practice as it's more convenient to implement, more efficient and owns stable performance.\n",
    "\n",
    "The Clipped Surrogated Objective approach replace the surrogate objective\n",
    "\n",
    "$$L^{CPI}(\\theta) = \\hat{\\mathbb{E}}_t[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t] = \\hat{\\mathbb{E}}_t[r_t(\\theta)\\hat{A}_t]$$\n",
    "\n",
    "of TRPO (CPI: Conservative Policy Iteration) by \n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta),1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
    "\n",
    "You can check that $L^{CLIP}(\\theta) = L^{CPI}(\\theta)$ around the old policy parameter $\\theta_{old}$, i.e., when r = 1.\n",
    "\n",
    "## TODOs here:\n",
    "\n",
    "In this section, your task is to finish the code of a PPO algorithm and evaluate its performance in the Pendulum-v0 environment.\n",
    "\n",
    "Specifically, you need to\n",
    "- Q1. finish building up the ActorCritic ''\\__init__'' function, i.e., build up the neural network.\n",
    "- Q2. finish the foward function, in this part, there are two functions need to finish: the \\_forward_actor function and the \\_forward_critic function\n",
    "- Q3. finish the select_action function, which is called during interacting with the environment, so that you may need to return an action as well as the (log-)probability of getting that action for future optimization\n",
    "- Q4. finish the optimization steps for your PPO agent, that means you need to build up the surrogate loss through your saved tuples in previous episodes and optimize it with current network parameters.\n",
    "- Q5. finally, you may need to optimize some of the hyper-parameters to have a better task performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hCkX9dWFvXxa"
   },
   "outputs": [],
   "source": [
    "# You need not to rivese this unless you want to try other hyper-parameter settings\n",
    "# in which case you may revise the default values of class args()\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as joindir\n",
    "from os import makedirs as mkdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'value', 'action', 'logproba', 'mask', 'next_state', 'reward'))\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()\n",
    "\n",
    "EPS = 1e-10 # you may need this tiny value somewhere, and think about why?\n",
    "RESULT_DIR = 'Result_PPO'\n",
    "mkdir(RESULT_DIR, exist_ok=True)\n",
    "mkdir(ENV_NAME.split('-')[0]+'/CheckPoints', exist_ok=True)\n",
    "mkdir(ENV_NAME.split('-')[0]+'/Rwds', exist_ok=True)\n",
    "rwds = []\n",
    "rwds_history = []\n",
    "\n",
    "class args(object):\n",
    "    hid_num = 256\n",
    "    drop_prob = 0.1\n",
    "    env_name = ENV_NAME\n",
    "    seed = 1234\n",
    "    num_episode = 1000\n",
    "    batch_size = 5120\n",
    "    max_step_per_round = 2000\n",
    "    gamma = 0.995\n",
    "    lamda = 0.97\n",
    "    log_num_episode = 1\n",
    "    num_epoch = 10\n",
    "    minibatch_size = 256\n",
    "    clip = 0.2\n",
    "    loss_coeff_value = 0.5\n",
    "    loss_coeff_entropy = 0.01\n",
    "    lr = 3e-4\n",
    "    num_parallel_run = 1\n",
    "    # tricks\n",
    "    schedule_adam = 'linear'\n",
    "    schedule_clip = 'linear'\n",
    "    layer_norm = True\n",
    "    state_norm = False\n",
    "    advantage_norm = True\n",
    "    lossvalue_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H0zwluYyx-y_"
   },
   "outputs": [],
   "source": [
    "# You need not to rivese this, these classes are used for normalization\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape):\n",
    "        self._n = 0\n",
    "        self._M = np.zeros(shape)\n",
    "        self._S = np.zeros(shape)\n",
    "\n",
    "    def push(self, x):\n",
    "        x = np.asarray(x)\n",
    "        assert x.shape == self._M.shape\n",
    "        self._n += 1\n",
    "        if self._n == 1:\n",
    "            self._M[...] = x\n",
    "        else:\n",
    "            oldM = self._M.copy()\n",
    "            self._M[...] = oldM + (x - oldM) / self._n\n",
    "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._n\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._M\n",
    "\n",
    "    @property\n",
    "    def var(self):\n",
    "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._M.shape\n",
    "\n",
    "\n",
    "class ZFilter:\n",
    "    \"\"\"\n",
    "    y = (x-mean)/std\n",
    "    using running estimates of mean,std\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
    "        self.demean = demean\n",
    "        self.destd = destd\n",
    "        self.clip = clip\n",
    "\n",
    "        self.rs = RunningStat(shape)\n",
    "\n",
    "    def __call__(self, x, update=True):\n",
    "        if update: self.rs.push(x)\n",
    "        if self.demean:\n",
    "            x = x - self.rs.mean\n",
    "        if self.destd:\n",
    "            x = x / (self.rs.std + 1e-8)\n",
    "        if self.clip:\n",
    "            x = np.clip(x, -self.clip, self.clip)\n",
    "        return x\n",
    "\n",
    "    def output_shape(self, input_space):\n",
    "        return input_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JDqYISHHyQve"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 0 Reward: -1237.4805 total_loss = 46.7091 = -0.2857 + 0.5 * 93.9911 + 0.01 * -0.0770\n",
      "-----------------\n",
      "Finished episode: 1 Reward: -1785.9216 total_loss = 31.4105 = -0.2742 + 0.5 * 63.3702 + 0.01 * -0.0357\n",
      "-----------------\n",
      "Finished episode: 2 Reward: -1476.4652 total_loss = 21.6972 = -0.4155 + 0.5 * 44.2266 + 0.01 * -0.0565\n",
      "-----------------\n",
      "Finished episode: 3 Reward: -1506.3947 total_loss = 28.3271 = -0.3391 + 0.5 * 57.3348 + 0.01 * -0.1185\n",
      "-----------------\n",
      "Finished episode: 4 Reward: -1351.1218 total_loss = 22.7980 = -0.6169 + 0.5 * 46.8309 + 0.01 * -0.0584\n",
      "-----------------\n",
      "Finished episode: 5 Reward: -1374.9007 total_loss = 24.2738 = -0.2735 + 0.5 * 49.0966 + 0.01 * -0.1041\n",
      "-----------------\n",
      "Finished episode: 6 Reward: -1404.0280 total_loss = 25.1256 = -0.5126 + 0.5 * 51.2772 + 0.01 * -0.0404\n",
      "-----------------\n",
      "Finished episode: 7 Reward: -1511.0193 total_loss = 25.7030 = -0.4036 + 0.5 * 52.2143 + 0.01 * -0.0507\n",
      "-----------------\n",
      "Finished episode: 8 Reward: -1504.0165 total_loss = 25.3226 = -0.2468 + 0.5 * 51.1390 + 0.01 * -0.0169\n",
      "-----------------\n",
      "Finished episode: 9 Reward: -1435.4967 total_loss = 21.8990 = -0.2869 + 0.5 * 44.3726 + 0.01 * -0.0405\n",
      "-----------------\n",
      "Finished episode: 10 Reward: -1523.8158 total_loss = 24.2283 = -0.9720 + 0.5 * 50.4017 + 0.01 * -0.0505\n",
      "-----------------\n",
      "Finished episode: 11 Reward: -1500.5717 total_loss = 24.0790 = -0.3705 + 0.5 * 48.8994 + 0.01 * -0.0292\n",
      "-----------------\n",
      "Finished episode: 12 Reward: -1428.1581 total_loss = 21.0465 = -0.3261 + 0.5 * 42.7466 + 0.01 * -0.0728\n",
      "-----------------\n",
      "Finished episode: 13 Reward: -1442.5342 total_loss = 22.1476 = -0.2838 + 0.5 * 44.8642 + 0.01 * -0.0713\n",
      "-----------------\n",
      "Finished episode: 14 Reward: -1485.6602 total_loss = 21.9320 = -0.2986 + 0.5 * 44.4633 + 0.01 * -0.0971\n",
      "-----------------\n",
      "Finished episode: 15 Reward: -1398.8174 total_loss = 20.0434 = -2.1650 + 0.5 * 44.4187 + 0.01 * -0.0910\n",
      "-----------------\n",
      "Finished episode: 16 Reward: -1443.3849 total_loss = 21.8296 = -0.3638 + 0.5 * 44.3896 + 0.01 * -0.1421\n",
      "-----------------\n",
      "Finished episode: 17 Reward: -1375.0947 total_loss = 21.0428 = -0.3955 + 0.5 * 42.8788 + 0.01 * -0.1145\n",
      "-----------------\n",
      "Finished episode: 18 Reward: -1453.6231 total_loss = 24.2116 = -0.2335 + 0.5 * 48.8918 + 0.01 * -0.0759\n",
      "-----------------\n",
      "Finished episode: 19 Reward: -1492.8306 total_loss = 23.7327 = -0.3170 + 0.5 * 48.1010 + 0.01 * -0.0804\n",
      "-----------------\n",
      "Finished episode: 20 Reward: -1491.7265 total_loss = 23.5852 = -0.3469 + 0.5 * 47.8668 + 0.01 * -0.1265\n",
      "-----------------\n",
      "Finished episode: 21 Reward: -1386.5772 total_loss = 21.7115 = -0.4196 + 0.5 * 44.2633 + 0.01 * -0.0493\n",
      "-----------------\n",
      "Finished episode: 22 Reward: -1495.7608 total_loss = 23.2617 = -0.3057 + 0.5 * 47.1357 + 0.01 * -0.0434\n",
      "-----------------\n",
      "Finished episode: 23 Reward: -1358.7360 total_loss = 20.3223 = -0.8947 + 0.5 * 42.4364 + 0.01 * -0.1144\n",
      "-----------------\n",
      "Finished episode: 24 Reward: -1450.7377 total_loss = 21.7816 = -0.2965 + 0.5 * 44.1589 + 0.01 * -0.1439\n",
      "-----------------\n",
      "Finished episode: 25 Reward: -1395.7394 total_loss = 19.3639 = -0.3727 + 0.5 * 39.4761 + 0.01 * -0.1433\n",
      "-----------------\n",
      "Finished episode: 26 Reward: -1356.4169 total_loss = 19.1616 = -0.4413 + 0.5 * 39.2072 + 0.01 * -0.0696\n",
      "-----------------\n",
      "Finished episode: 27 Reward: -1392.3519 total_loss = 21.3714 = -0.4667 + 0.5 * 43.6788 + 0.01 * -0.1234\n",
      "-----------------\n",
      "Finished episode: 28 Reward: -1488.0155 total_loss = 22.6168 = -0.3481 + 0.5 * 45.9328 + 0.01 * -0.1492\n",
      "-----------------\n",
      "Finished episode: 29 Reward: -1414.2907 total_loss = 21.8398 = -0.7382 + 0.5 * 45.1580 + 0.01 * -0.1078\n",
      "-----------------\n",
      "Finished episode: 30 Reward: -1483.2695 total_loss = 22.8146 = -0.3950 + 0.5 * 46.4216 + 0.01 * -0.1200\n",
      "-----------------\n",
      "Finished episode: 31 Reward: -1436.1648 total_loss = 23.3282 = -0.3034 + 0.5 * 47.2653 + 0.01 * -0.1071\n",
      "-----------------\n",
      "Finished episode: 32 Reward: -1427.2166 total_loss = 20.9784 = -0.5075 + 0.5 * 42.9746 + 0.01 * -0.1403\n",
      "-----------------\n",
      "Finished episode: 33 Reward: -1453.6650 total_loss = 22.2199 = -0.3601 + 0.5 * 45.1619 + 0.01 * -0.0898\n",
      "-----------------\n",
      "Finished episode: 34 Reward: -1401.4391 total_loss = 20.0736 = -0.5093 + 0.5 * 41.1668 + 0.01 * -0.0489\n",
      "-----------------\n",
      "Finished episode: 35 Reward: -1398.7571 total_loss = 22.3861 = -0.9006 + 0.5 * 46.5750 + 0.01 * -0.0782\n",
      "-----------------\n",
      "Finished episode: 36 Reward: -1487.2886 total_loss = 23.0459 = -0.3767 + 0.5 * 46.8480 + 0.01 * -0.1324\n",
      "-----------------\n",
      "Finished episode: 37 Reward: -1513.6194 total_loss = 22.5780 = -0.7011 + 0.5 * 46.5612 + 0.01 * -0.1455\n",
      "-----------------\n",
      "Finished episode: 38 Reward: -1483.0669 total_loss = 22.2328 = -0.3255 + 0.5 * 45.1180 + 0.01 * -0.0670\n",
      "-----------------\n",
      "Finished episode: 39 Reward: -1650.3763 total_loss = 24.1793 = -0.3462 + 0.5 * 49.0531 + 0.01 * -0.1046\n",
      "-----------------\n",
      "Finished episode: 40 Reward: -1451.4352 total_loss = 11.6506 = -0.3327 + 0.5 * 23.9685 + 0.01 * -0.0984\n",
      "-----------------\n",
      "Finished episode: 41 Reward: -1249.3387 total_loss = 19.6227 = -0.5141 + 0.5 * 40.2753 + 0.01 * -0.0874\n",
      "-----------------\n",
      "Finished episode: 42 Reward: -1243.2832 total_loss = 21.1439 = -0.3769 + 0.5 * 43.0427 + 0.01 * -0.0507\n",
      "-----------------\n",
      "Finished episode: 43 Reward: -1486.2350 total_loss = 23.2738 = -0.4118 + 0.5 * 47.3723 + 0.01 * -0.0510\n",
      "-----------------\n",
      "Finished episode: 44 Reward: -1568.9352 total_loss = 17.6898 = -0.4698 + 0.5 * 36.3195 + 0.01 * -0.0201\n",
      "-----------------\n",
      "Finished episode: 45 Reward: -1370.1011 total_loss = 17.7187 = -0.6204 + 0.5 * 36.6804 + 0.01 * -0.1035\n",
      "-----------------\n",
      "Finished episode: 46 Reward: -1458.8156 total_loss = 18.1187 = -0.3861 + 0.5 * 37.0112 + 0.01 * -0.0819\n",
      "-----------------\n",
      "Finished episode: 47 Reward: -1460.3950 total_loss = 15.5747 = -0.6866 + 0.5 * 32.5243 + 0.01 * -0.0793\n",
      "-----------------\n",
      "Finished episode: 48 Reward: -1266.2985 total_loss = 19.0710 = -0.6662 + 0.5 * 39.4762 + 0.01 * -0.0816\n",
      "-----------------\n",
      "Finished episode: 49 Reward: -1224.5985 total_loss = 16.9903 = -0.5963 + 0.5 * 35.1748 + 0.01 * -0.0792\n",
      "-----------------\n",
      "Finished episode: 50 Reward: -1189.5963 total_loss = 17.9416 = -0.4101 + 0.5 * 36.7036 + 0.01 * -0.0141\n",
      "-----------------\n",
      "Finished episode: 51 Reward: -1446.0666 total_loss = 19.1078 = -0.5957 + 0.5 * 39.4093 + 0.01 * -0.1127\n",
      "-----------------\n",
      "Finished episode: 52 Reward: -1446.5547 total_loss = 15.6903 = -0.3097 + 0.5 * 32.0006 + 0.01 * -0.0344\n",
      "-----------------\n",
      "Finished episode: 53 Reward: -1367.0152 total_loss = 16.4777 = -0.2901 + 0.5 * 33.5355 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 54 Reward: -1382.8434 total_loss = 22.0496 = -0.4012 + 0.5 * 44.9024 + 0.01 * -0.0385\n",
      "-----------------\n",
      "Finished episode: 55 Reward: -1437.7374 total_loss = 22.1890 = -0.2996 + 0.5 * 44.9781 + 0.01 * -0.0420\n",
      "-----------------\n",
      "Finished episode: 56 Reward: -1423.7451 total_loss = 21.8464 = -0.2528 + 0.5 * 44.1992 + 0.01 * -0.0363\n",
      "-----------------\n",
      "Finished episode: 57 Reward: -1461.0456 total_loss = 22.4131 = -0.2790 + 0.5 * 45.3850 + 0.01 * -0.0399\n",
      "-----------------\n",
      "Finished episode: 58 Reward: -1412.9380 total_loss = 21.5465 = -0.3430 + 0.5 * 43.7800 + 0.01 * -0.0507\n",
      "-----------------\n",
      "Finished episode: 59 Reward: -1471.2899 total_loss = 22.6228 = -0.3595 + 0.5 * 45.9653 + 0.01 * -0.0315\n",
      "-----------------\n",
      "Finished episode: 60 Reward: -1483.6581 total_loss = 22.8121 = -0.3145 + 0.5 * 46.2540 + 0.01 * -0.0334\n",
      "-----------------\n",
      "Finished episode: 61 Reward: -1450.3142 total_loss = 22.8862 = -0.3377 + 0.5 * 46.4484 + 0.01 * -0.0317\n",
      "-----------------\n",
      "Finished episode: 62 Reward: -1409.1062 total_loss = 21.4898 = -0.4598 + 0.5 * 43.8989 + 0.01 * 0.0098\n",
      "-----------------\n",
      "Finished episode: 63 Reward: -1375.8231 total_loss = 22.3052 = -0.2795 + 0.5 * 45.1696 + 0.01 * -0.0138\n",
      "-----------------\n",
      "Finished episode: 64 Reward: -1354.4080 total_loss = 20.5939 = -0.2751 + 0.5 * 41.7392 + 0.01 * -0.0568\n",
      "-----------------\n",
      "Finished episode: 65 Reward: -1460.9731 total_loss = 21.1032 = -0.3105 + 0.5 * 42.8278 + 0.01 * -0.0183\n",
      "-----------------\n",
      "Finished episode: 66 Reward: -1441.0354 total_loss = 21.1117 = -0.2997 + 0.5 * 42.8249 + 0.01 * -0.1028\n",
      "-----------------\n",
      "Finished episode: 67 Reward: -1387.6689 total_loss = 20.9377 = -0.3653 + 0.5 * 42.6073 + 0.01 * -0.0634\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 68 Reward: -1429.1425 total_loss = 22.2076 = -0.2527 + 0.5 * 44.9199 + 0.01 * 0.0299\n",
      "-----------------\n",
      "Finished episode: 69 Reward: -1452.6503 total_loss = 20.9555 = -0.2957 + 0.5 * 42.5026 + 0.01 * -0.0077\n",
      "-----------------\n",
      "Finished episode: 70 Reward: -1402.5377 total_loss = 21.7817 = -0.4631 + 0.5 * 44.4890 + 0.01 * 0.0376\n",
      "-----------------\n",
      "Finished episode: 71 Reward: -1420.2661 total_loss = 21.7473 = -0.2988 + 0.5 * 44.0925 + 0.01 * -0.0128\n",
      "-----------------\n",
      "Finished episode: 72 Reward: -1433.3672 total_loss = 20.7301 = -0.3602 + 0.5 * 42.1825 + 0.01 * -0.0942\n",
      "-----------------\n",
      "Finished episode: 73 Reward: -1392.5366 total_loss = 19.5742 = -0.2782 + 0.5 * 39.7038 + 0.01 * 0.0452\n",
      "-----------------\n",
      "Finished episode: 74 Reward: -1331.6514 total_loss = 19.0632 = -0.3031 + 0.5 * 38.7326 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 75 Reward: -1467.3975 total_loss = 21.3292 = -0.3788 + 0.5 * 43.4154 + 0.01 * 0.0268\n",
      "-----------------\n",
      "Finished episode: 76 Reward: -1498.5559 total_loss = 19.8751 = -0.4053 + 0.5 * 40.5614 + 0.01 * -0.0351\n",
      "-----------------\n",
      "Finished episode: 77 Reward: -1430.3990 total_loss = 20.0714 = -0.4901 + 0.5 * 41.1238 + 0.01 * -0.0435\n",
      "-----------------\n",
      "Finished episode: 78 Reward: -1489.3662 total_loss = 19.3141 = -0.3339 + 0.5 * 39.2965 + 0.01 * -0.0204\n",
      "-----------------\n",
      "Finished episode: 79 Reward: -1451.0951 total_loss = 20.7590 = -0.2734 + 0.5 * 42.0654 + 0.01 * -0.0220\n",
      "-----------------\n",
      "Finished episode: 80 Reward: -1436.9302 total_loss = 22.3389 = -0.2918 + 0.5 * 45.2608 + 0.01 * 0.0354\n",
      "-----------------\n",
      "Finished episode: 81 Reward: -1455.0143 total_loss = 20.6763 = -0.4426 + 0.5 * 42.2383 + 0.01 * -0.0181\n",
      "-----------------\n",
      "Finished episode: 82 Reward: -1407.2429 total_loss = 20.4632 = -0.4106 + 0.5 * 41.7495 + 0.01 * -0.0889\n",
      "-----------------\n",
      "Finished episode: 83 Reward: -1355.4546 total_loss = 19.1657 = -0.2817 + 0.5 * 38.8961 + 0.01 * -0.0653\n",
      "-----------------\n",
      "Finished episode: 84 Reward: -1417.7609 total_loss = 19.3786 = -0.3795 + 0.5 * 39.5165 + 0.01 * -0.0187\n",
      "-----------------\n",
      "Finished episode: 85 Reward: -1474.5080 total_loss = 21.7702 = -0.3573 + 0.5 * 44.2540 + 0.01 * 0.0489\n",
      "-----------------\n",
      "Finished episode: 86 Reward: -1455.1079 total_loss = 20.8397 = -0.2994 + 0.5 * 42.2782 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 87 Reward: -1468.9601 total_loss = 20.8415 = -0.2589 + 0.5 * 42.2018 + 0.01 * -0.0574\n",
      "-----------------\n",
      "Finished episode: 88 Reward: -1448.8763 total_loss = 20.3787 = -0.3953 + 0.5 * 41.5485 + 0.01 * -0.0226\n",
      "-----------------\n",
      "Finished episode: 89 Reward: -1483.3137 total_loss = 19.7557 = -0.3323 + 0.5 * 40.1745 + 0.01 * 0.0827\n",
      "-----------------\n",
      "Finished episode: 90 Reward: -1411.3619 total_loss = 21.8601 = -0.2512 + 0.5 * 44.2229 + 0.01 * -0.0169\n",
      "-----------------\n",
      "Finished episode: 91 Reward: -1383.0256 total_loss = 20.7519 = -0.2869 + 0.5 * 42.0771 + 0.01 * 0.0236\n",
      "-----------------\n",
      "Finished episode: 92 Reward: -1390.1453 total_loss = 19.2849 = -0.2923 + 0.5 * 39.1525 + 0.01 * 0.1059\n",
      "-----------------\n",
      "Finished episode: 93 Reward: -1397.0239 total_loss = 19.8603 = -0.2920 + 0.5 * 40.3045 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 94 Reward: -1359.0515 total_loss = 19.1972 = -0.3184 + 0.5 * 39.0300 + 0.01 * 0.0649\n",
      "-----------------\n",
      "Finished episode: 95 Reward: -1418.9761 total_loss = 20.1428 = -0.3355 + 0.5 * 40.9569 + 0.01 * -0.0199\n",
      "-----------------\n",
      "Finished episode: 96 Reward: -1465.5618 total_loss = 21.3776 = -0.3229 + 0.5 * 43.4009 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 97 Reward: -1413.3191 total_loss = 19.8765 = -0.3118 + 0.5 * 40.3761 + 0.01 * 0.0272\n",
      "-----------------\n",
      "Finished episode: 98 Reward: -1426.0697 total_loss = 19.7437 = -0.3408 + 0.5 * 40.1656 + 0.01 * 0.1696\n",
      "-----------------\n",
      "Finished episode: 99 Reward: -1423.2955 total_loss = 22.2215 = -0.3482 + 0.5 * 45.1359 + 0.01 * 0.1773\n",
      "-----------------\n",
      "Finished episode: 100 Reward: -1408.8297 total_loss = 19.4318 = -0.3022 + 0.5 * 39.4681 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 101 Reward: -1420.8564 total_loss = 18.8287 = -0.2900 + 0.5 * 38.2374 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 102 Reward: -1329.9666 total_loss = 21.0074 = -0.2913 + 0.5 * 42.5967 + 0.01 * 0.0316\n",
      "-----------------\n",
      "Finished episode: 103 Reward: -1467.4983 total_loss = 18.5974 = -0.3153 + 0.5 * 37.8254 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 104 Reward: -1454.1725 total_loss = 21.5588 = -0.2839 + 0.5 * 43.6853 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 105 Reward: -1464.0703 total_loss = 21.2176 = -0.2883 + 0.5 * 43.0111 + 0.01 * 0.0285\n",
      "-----------------\n",
      "Finished episode: 106 Reward: -1476.6630 total_loss = 19.3988 = -0.2951 + 0.5 * 39.3877 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 107 Reward: -1431.3975 total_loss = 19.2752 = -0.3100 + 0.5 * 39.1703 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 108 Reward: -1415.8919 total_loss = 20.1598 = -0.2773 + 0.5 * 40.8742 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 109 Reward: -1365.3700 total_loss = 19.1549 = -0.3024 + 0.5 * 38.9145 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 110 Reward: -1389.5409 total_loss = 20.1517 = -0.3100 + 0.5 * 40.9234 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 111 Reward: -1481.2058 total_loss = 20.4378 = -0.2897 + 0.5 * 41.4550 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 112 Reward: -1530.1651 total_loss = 21.2927 = -0.3308 + 0.5 * 43.2471 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 113 Reward: -1373.5487 total_loss = 18.2577 = -0.2879 + 0.5 * 37.0911 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 114 Reward: -1517.4799 total_loss = 20.7813 = -0.3015 + 0.5 * 42.1656 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 115 Reward: -1445.9314 total_loss = 19.4856 = -0.2799 + 0.5 * 39.5309 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 116 Reward: -1467.4355 total_loss = 20.4448 = -0.4016 + 0.5 * 41.6898 + 0.01 * 0.1565\n",
      "-----------------\n",
      "Finished episode: 117 Reward: -1367.7117 total_loss = 16.4146 = -0.3065 + 0.5 * 33.4423 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 118 Reward: -1391.9311 total_loss = 19.5741 = -0.3063 + 0.5 * 39.7608 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 119 Reward: -1350.6564 total_loss = 19.2149 = -0.2681 + 0.5 * 38.9659 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 120 Reward: -1508.2762 total_loss = 20.0465 = -0.2818 + 0.5 * 40.6566 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 121 Reward: -1410.9821 total_loss = 18.3485 = -0.3137 + 0.5 * 37.3243 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 122 Reward: -1398.2955 total_loss = 20.4381 = -0.2902 + 0.5 * 41.4561 + 0.01 * 0.0260\n",
      "-----------------\n",
      "Finished episode: 123 Reward: -1407.7068 total_loss = 19.2909 = -0.3092 + 0.5 * 39.2001 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 124 Reward: -1298.3262 total_loss = 18.4564 = -0.2773 + 0.5 * 37.4673 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 125 Reward: -1408.9593 total_loss = 19.8111 = -0.2938 + 0.5 * 40.2092 + 0.01 * 0.0315\n",
      "-----------------\n",
      "Finished episode: 126 Reward: -1393.4222 total_loss = 18.4183 = -0.2801 + 0.5 * 37.3958 + 0.01 * 0.0487\n",
      "-----------------\n",
      "Finished episode: 127 Reward: -1466.5385 total_loss = 19.9685 = -0.2756 + 0.5 * 40.4882 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 128 Reward: -1355.7909 total_loss = 18.3237 = -0.2975 + 0.5 * 37.2421 + 0.01 * 0.0169\n",
      "-----------------\n",
      "Finished episode: 129 Reward: -1389.1047 total_loss = 17.1613 = -0.3119 + 0.5 * 34.9455 + 0.01 * 0.0427\n",
      "-----------------\n",
      "Finished episode: 130 Reward: -1413.5759 total_loss = 18.3836 = -0.2969 + 0.5 * 37.3609 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 131 Reward: -1452.0887 total_loss = 20.2096 = -0.2918 + 0.5 * 41.0027 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 132 Reward: -1437.7915 total_loss = 18.3088 = -0.3004 + 0.5 * 37.2184 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 133 Reward: -1513.0268 total_loss = 20.5835 = -0.2859 + 0.5 * 41.7388 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 134 Reward: -1437.5703 total_loss = 20.0807 = -0.2852 + 0.5 * 40.7318 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 135 Reward: -1350.1287 total_loss = 19.8261 = -0.3214 + 0.5 * 40.2932 + 0.01 * 0.0913\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 136 Reward: -1467.7980 total_loss = 20.8272 = -0.2902 + 0.5 * 42.2348 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 137 Reward: -1450.2422 total_loss = 19.3711 = -0.3065 + 0.5 * 39.3553 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 138 Reward: -1428.2647 total_loss = 19.1088 = -0.2945 + 0.5 * 38.8052 + 0.01 * 0.0760\n",
      "-----------------\n",
      "Finished episode: 139 Reward: -1414.3741 total_loss = 20.0273 = -0.2768 + 0.5 * 40.6081 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 140 Reward: -1407.9820 total_loss = 19.1433 = -0.3152 + 0.5 * 38.9157 + 0.01 * 0.0670\n",
      "-----------------\n",
      "Finished episode: 141 Reward: -1342.6159 total_loss = 18.2796 = -0.2948 + 0.5 * 37.1489 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 142 Reward: -1452.1412 total_loss = 20.1606 = -0.2830 + 0.5 * 40.8872 + 0.01 * -0.0003\n",
      "-----------------\n",
      "Finished episode: 143 Reward: -1371.3782 total_loss = 17.4013 = -0.3063 + 0.5 * 35.4153 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 144 Reward: -1415.4799 total_loss = 19.0485 = -0.2977 + 0.5 * 38.6925 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 145 Reward: -1478.5495 total_loss = 20.2900 = -0.3021 + 0.5 * 41.1831 + 0.01 * 0.0499\n",
      "-----------------\n",
      "Finished episode: 146 Reward: -1434.6840 total_loss = 21.0696 = -0.3007 + 0.5 * 42.7399 + 0.01 * 0.0249\n",
      "-----------------\n",
      "Finished episode: 147 Reward: -1450.7810 total_loss = 21.3975 = -0.2733 + 0.5 * 43.3416 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 148 Reward: -1449.1092 total_loss = 20.4531 = -0.2884 + 0.5 * 41.4829 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 149 Reward: -1355.2339 total_loss = 18.9047 = -0.2951 + 0.5 * 38.3997 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 150 Reward: -1461.1970 total_loss = 19.1431 = -0.2684 + 0.5 * 38.8230 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 151 Reward: -1394.5581 total_loss = 18.4362 = -0.3004 + 0.5 * 37.4732 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 152 Reward: -1515.2356 total_loss = 22.4636 = -0.2747 + 0.5 * 45.4750 + 0.01 * 0.0805\n",
      "-----------------\n",
      "Finished episode: 153 Reward: -1482.0665 total_loss = 20.4182 = -0.2911 + 0.5 * 41.4169 + 0.01 * 0.0875\n",
      "-----------------\n",
      "Finished episode: 154 Reward: -1480.5307 total_loss = 20.6416 = -0.3106 + 0.5 * 41.9029 + 0.01 * 0.0777\n",
      "-----------------\n",
      "Finished episode: 155 Reward: -1469.8032 total_loss = 21.4265 = -0.3414 + 0.5 * 43.5356 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 156 Reward: -1488.8233 total_loss = 21.4905 = -0.2766 + 0.5 * 43.5324 + 0.01 * 0.0942\n",
      "-----------------\n",
      "Finished episode: 157 Reward: -1429.6200 total_loss = 20.8435 = -0.3137 + 0.5 * 42.3143 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 158 Reward: -1445.2129 total_loss = 20.4507 = -0.2822 + 0.5 * 41.4658 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 159 Reward: -1451.3616 total_loss = 18.7445 = -0.2842 + 0.5 * 38.0553 + 0.01 * 0.1013\n",
      "-----------------\n",
      "Finished episode: 160 Reward: -1432.0436 total_loss = 20.1549 = -0.3278 + 0.5 * 40.9653 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 161 Reward: -1455.4190 total_loss = 19.4292 = -0.2738 + 0.5 * 39.4060 + 0.01 * 0.0028\n",
      "-----------------\n",
      "Finished episode: 162 Reward: -1426.2182 total_loss = 18.7632 = -0.3043 + 0.5 * 38.1338 + 0.01 * 0.0567\n",
      "-----------------\n",
      "Finished episode: 163 Reward: -1358.2378 total_loss = 17.7043 = -0.3062 + 0.5 * 36.0209 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 164 Reward: -1420.3578 total_loss = 18.8711 = -0.2917 + 0.5 * 38.3255 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 165 Reward: -1461.1704 total_loss = 19.7001 = -0.3052 + 0.5 * 40.0105 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 166 Reward: -1462.7441 total_loss = 20.3246 = -0.3000 + 0.5 * 41.2492 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 167 Reward: -1433.6444 total_loss = 20.6291 = -0.2849 + 0.5 * 41.8271 + 0.01 * 0.0420\n",
      "-----------------\n",
      "Finished episode: 168 Reward: -1506.3912 total_loss = 18.8640 = -0.2706 + 0.5 * 38.2692 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 169 Reward: -1497.3752 total_loss = 20.5761 = -0.3071 + 0.5 * 41.7654 + 0.01 * 0.0461\n",
      "-----------------\n",
      "Finished episode: 170 Reward: -1435.8909 total_loss = 19.5364 = -0.3980 + 0.5 * 39.8555 + 0.01 * 0.6597\n",
      "-----------------\n",
      "Finished episode: 171 Reward: -1441.6858 total_loss = 20.4654 = -0.2751 + 0.5 * 41.4799 + 0.01 * 0.0608\n",
      "-----------------\n",
      "Finished episode: 172 Reward: -1381.7512 total_loss = 18.2367 = -0.2794 + 0.5 * 37.0323 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 173 Reward: -1455.6302 total_loss = 19.0365 = -0.2991 + 0.5 * 38.6705 + 0.01 * 0.0443\n",
      "-----------------\n",
      "Finished episode: 174 Reward: -1461.7580 total_loss = 19.4190 = -0.3037 + 0.5 * 39.4453 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 175 Reward: -1437.1780 total_loss = 19.4423 = -0.3006 + 0.5 * 39.4857 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 176 Reward: -1290.7073 total_loss = 17.8050 = -0.3075 + 0.5 * 36.2234 + 0.01 * 0.0759\n",
      "-----------------\n",
      "Finished episode: 177 Reward: -1354.1594 total_loss = 18.7426 = -0.3005 + 0.5 * 38.0861 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 178 Reward: -1475.8243 total_loss = 19.3911 = -0.2885 + 0.5 * 39.3591 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 179 Reward: -1477.6500 total_loss = 21.4930 = -0.3011 + 0.5 * 43.5844 + 0.01 * 0.1981\n",
      "-----------------\n",
      "Finished episode: 180 Reward: -1481.5408 total_loss = 20.3368 = -0.3222 + 0.5 * 41.3143 + 0.01 * 0.1868\n",
      "-----------------\n",
      "Finished episode: 181 Reward: -1415.2418 total_loss = 18.0116 = -0.3074 + 0.5 * 36.6379 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 182 Reward: -1425.6322 total_loss = 18.8561 = -0.2998 + 0.5 * 38.3117 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 183 Reward: -1518.5373 total_loss = 20.4625 = -0.3086 + 0.5 * 41.5422 + 0.01 * 0.0040\n",
      "-----------------\n",
      "Finished episode: 184 Reward: -1445.4094 total_loss = 19.7836 = -0.3059 + 0.5 * 40.1769 + 0.01 * 0.1124\n",
      "-----------------\n",
      "Finished episode: 185 Reward: -1389.4101 total_loss = 18.3380 = -0.3128 + 0.5 * 37.3017 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 186 Reward: -1459.2924 total_loss = 19.1510 = -0.2812 + 0.5 * 38.8639 + 0.01 * 0.0219\n",
      "-----------------\n",
      "Finished episode: 187 Reward: -1400.5468 total_loss = 19.7886 = -0.3220 + 0.5 * 40.2213 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 188 Reward: -1470.8419 total_loss = 20.2247 = -0.2937 + 0.5 * 41.0367 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 189 Reward: -1420.4076 total_loss = 18.9850 = -0.2754 + 0.5 * 38.5208 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 190 Reward: -1358.1600 total_loss = 20.8203 = -0.2767 + 0.5 * 42.1940 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 191 Reward: -1443.4857 total_loss = 19.5482 = -0.3091 + 0.5 * 39.7145 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 192 Reward: -1431.4216 total_loss = 20.4456 = -0.2943 + 0.5 * 41.4774 + 0.01 * 0.1181\n",
      "-----------------\n",
      "Finished episode: 193 Reward: -1533.5403 total_loss = 20.9070 = -0.3003 + 0.5 * 42.4146 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 194 Reward: -1480.6745 total_loss = 21.5338 = -0.2820 + 0.5 * 43.6316 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 195 Reward: -1432.6735 total_loss = 18.9063 = -0.3051 + 0.5 * 38.4211 + 0.01 * 0.0765\n",
      "-----------------\n",
      "Finished episode: 196 Reward: -1438.3172 total_loss = 18.1179 = -0.3062 + 0.5 * 36.8482 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 197 Reward: -1430.2605 total_loss = 19.2831 = -0.3048 + 0.5 * 39.1759 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 198 Reward: -1447.2848 total_loss = 19.5841 = -0.3002 + 0.5 * 39.7674 + 0.01 * 0.0575\n",
      "-----------------\n",
      "Finished episode: 199 Reward: -1414.6417 total_loss = 19.7829 = -0.2889 + 0.5 * 40.1428 + 0.01 * 0.0350\n",
      "-----------------\n",
      "Finished episode: 200 Reward: -1310.9082 total_loss = 19.6120 = -0.2997 + 0.5 * 39.8202 + 0.01 * 0.1595\n",
      "-----------------\n",
      "Finished episode: 201 Reward: -1448.2529 total_loss = 21.0342 = -0.3103 + 0.5 * 42.6889 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 202 Reward: -1430.5929 total_loss = 20.1732 = -0.2785 + 0.5 * 40.9033 + 0.01 * 0.0000\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 203 Reward: -1437.6504 total_loss = 20.1067 = -0.2716 + 0.5 * 40.7553 + 0.01 * 0.0660\n",
      "-----------------\n",
      "Finished episode: 204 Reward: -1484.3251 total_loss = 19.5058 = -0.2771 + 0.5 * 39.5657 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 205 Reward: -1427.2525 total_loss = 18.1607 = -0.2878 + 0.5 * 36.8970 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 206 Reward: -1394.2169 total_loss = 18.1382 = -0.2906 + 0.5 * 36.8575 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 207 Reward: -1472.6660 total_loss = 20.6506 = -0.3041 + 0.5 * 41.9090 + 0.01 * 0.0222\n",
      "-----------------\n",
      "Finished episode: 208 Reward: -1417.2401 total_loss = 20.0354 = -0.3063 + 0.5 * 40.6835 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 209 Reward: -1451.7513 total_loss = 20.1160 = -0.3299 + 0.5 * 40.8912 + 0.01 * 0.0329\n",
      "-----------------\n",
      "Finished episode: 210 Reward: -1474.6890 total_loss = 18.6364 = -0.3283 + 0.5 * 37.9280 + 0.01 * 0.0644\n",
      "-----------------\n",
      "Finished episode: 211 Reward: -1419.6501 total_loss = 18.5817 = -0.3183 + 0.5 * 37.8000 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 212 Reward: -1410.8562 total_loss = 18.9694 = -0.3202 + 0.5 * 38.5793 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 213 Reward: -1438.3034 total_loss = 20.6141 = -0.2985 + 0.5 * 41.8252 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 214 Reward: -1450.8855 total_loss = 21.3080 = -0.3018 + 0.5 * 43.2196 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 215 Reward: -1397.2742 total_loss = 20.5242 = -0.2632 + 0.5 * 41.5747 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 216 Reward: -1514.9666 total_loss = 21.0443 = -0.2870 + 0.5 * 42.6613 + 0.01 * 0.0531\n",
      "-----------------\n",
      "Finished episode: 217 Reward: -1355.0738 total_loss = 18.3715 = -0.3036 + 0.5 * 37.3502 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 218 Reward: -1432.7428 total_loss = 19.2628 = -0.3199 + 0.5 * 39.1652 + 0.01 * 0.0158\n",
      "-----------------\n",
      "Finished episode: 219 Reward: -1495.4571 total_loss = 20.6405 = -0.3021 + 0.5 * 41.8852 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 220 Reward: -1562.4443 total_loss = 20.7712 = -0.2945 + 0.5 * 42.1313 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 221 Reward: -1443.9899 total_loss = 19.3730 = -0.3137 + 0.5 * 39.3734 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 222 Reward: -1341.8269 total_loss = 18.6177 = -0.2888 + 0.5 * 37.8131 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 223 Reward: -1402.3182 total_loss = 19.6218 = -0.3142 + 0.5 * 39.8721 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 224 Reward: -1379.4908 total_loss = 18.5391 = -0.2939 + 0.5 * 37.6645 + 0.01 * 0.0750\n",
      "-----------------\n",
      "Finished episode: 225 Reward: -1454.5556 total_loss = 19.4143 = -0.3122 + 0.5 * 39.4529 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 226 Reward: -1446.2474 total_loss = 20.4492 = -0.3083 + 0.5 * 41.5148 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 227 Reward: -1397.5461 total_loss = 18.8091 = -0.2901 + 0.5 * 38.1984 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 228 Reward: -1441.1467 total_loss = 19.8210 = -0.2952 + 0.5 * 40.2324 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 229 Reward: -1438.2076 total_loss = 19.6853 = -0.2929 + 0.5 * 39.9564 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 230 Reward: -1440.6701 total_loss = 20.3733 = -0.2845 + 0.5 * 41.3156 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 231 Reward: -1459.8903 total_loss = 21.6442 = -0.2707 + 0.5 * 43.8297 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 232 Reward: -1370.0023 total_loss = 20.3117 = -0.3157 + 0.5 * 41.2549 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 233 Reward: -1385.7629 total_loss = 18.0409 = -0.2916 + 0.5 * 36.6651 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 234 Reward: -1414.0754 total_loss = 18.6359 = -0.3252 + 0.5 * 37.9223 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 235 Reward: -1412.5983 total_loss = 19.8309 = -0.2903 + 0.5 * 40.2423 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 236 Reward: -1356.3401 total_loss = 18.6537 = -0.2788 + 0.5 * 37.8650 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 237 Reward: -1421.7060 total_loss = 18.3067 = -0.3180 + 0.5 * 37.2495 + 0.01 * -0.0007\n",
      "-----------------\n",
      "Finished episode: 238 Reward: -1431.9291 total_loss = 19.1394 = -0.2914 + 0.5 * 38.8615 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 239 Reward: -1422.2685 total_loss = 19.9923 = -0.2865 + 0.5 * 40.5576 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 240 Reward: -1367.0552 total_loss = 18.7927 = -0.2914 + 0.5 * 38.1682 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 241 Reward: -1429.7622 total_loss = 19.5867 = -0.2963 + 0.5 * 39.7659 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 242 Reward: -1408.3039 total_loss = 18.0079 = -0.2957 + 0.5 * 36.6049 + 0.01 * 0.1238\n",
      "-----------------\n",
      "Finished episode: 243 Reward: -1473.9282 total_loss = 19.1516 = -0.2860 + 0.5 * 38.8754 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 244 Reward: -1432.9204 total_loss = 20.1514 = -0.3049 + 0.5 * 40.9127 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 245 Reward: -1427.6481 total_loss = 18.1535 = -0.3012 + 0.5 * 36.9080 + 0.01 * 0.0678\n",
      "-----------------\n",
      "Finished episode: 246 Reward: -1465.7733 total_loss = 20.5370 = -0.2872 + 0.5 * 41.6485 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 247 Reward: -1429.4154 total_loss = 19.3396 = -0.3232 + 0.5 * 39.3219 + 0.01 * 0.1812\n",
      "-----------------\n",
      "Finished episode: 248 Reward: -1379.2071 total_loss = 19.5160 = -0.3075 + 0.5 * 39.6470 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 249 Reward: -1426.1567 total_loss = 18.9994 = -0.3106 + 0.5 * 38.6200 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 250 Reward: -1431.7413 total_loss = 20.3537 = -0.2905 + 0.5 * 41.2884 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 251 Reward: -1409.4488 total_loss = 19.9487 = -0.2771 + 0.5 * 40.4515 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 252 Reward: -1430.4122 total_loss = 21.7408 = -0.2687 + 0.5 * 44.0192 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 253 Reward: -1470.4473 total_loss = 21.7838 = -0.3211 + 0.5 * 44.2098 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 254 Reward: -1367.7698 total_loss = 17.5966 = -0.3029 + 0.5 * 35.7991 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 255 Reward: -1381.1274 total_loss = 17.4437 = -0.3128 + 0.5 * 35.5130 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 256 Reward: -1392.1216 total_loss = 18.2934 = -0.2960 + 0.5 * 37.1773 + 0.01 * 0.0695\n",
      "-----------------\n",
      "Finished episode: 257 Reward: -1455.6521 total_loss = 19.9824 = -0.2978 + 0.5 * 40.5605 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 258 Reward: -1346.1400 total_loss = 19.4125 = -0.3063 + 0.5 * 39.4368 + 0.01 * 0.0368\n",
      "-----------------\n",
      "Finished episode: 259 Reward: -1513.9427 total_loss = 20.1101 = -0.3005 + 0.5 * 40.8210 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 260 Reward: -1441.4618 total_loss = 18.0171 = -0.3038 + 0.5 * 36.6418 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 261 Reward: -1472.2369 total_loss = 20.2493 = -0.3164 + 0.5 * 41.1314 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 262 Reward: -1410.5447 total_loss = 18.2060 = -0.3008 + 0.5 * 37.0111 + 0.01 * 0.1319\n",
      "-----------------\n",
      "Finished episode: 263 Reward: -1417.9873 total_loss = 20.4185 = -0.3000 + 0.5 * 41.4369 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 264 Reward: -1450.3929 total_loss = 20.3306 = -0.2701 + 0.5 * 41.2012 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 265 Reward: -1452.9415 total_loss = 19.8592 = -0.2915 + 0.5 * 40.3014 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 266 Reward: -1406.7940 total_loss = 19.1675 = -0.3002 + 0.5 * 38.9348 + 0.01 * 0.0315\n",
      "-----------------\n",
      "Finished episode: 267 Reward: -1426.2516 total_loss = 20.0620 = -0.3107 + 0.5 * 40.7455 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 268 Reward: -1371.6239 total_loss = 18.8326 = -0.3093 + 0.5 * 38.2838 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 269 Reward: -1460.4743 total_loss = 20.3918 = -0.3044 + 0.5 * 41.3926 + 0.01 * -0.0000\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 270 Reward: -1426.9853 total_loss = 18.6047 = -0.3113 + 0.5 * 37.8304 + 0.01 * 0.0746\n",
      "-----------------\n",
      "Finished episode: 271 Reward: -1436.3296 total_loss = 19.8261 = -0.2571 + 0.5 * 40.1665 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 272 Reward: -1468.3466 total_loss = 19.7641 = -0.2723 + 0.5 * 40.0703 + 0.01 * 0.1264\n",
      "-----------------\n",
      "Finished episode: 273 Reward: -1481.2083 total_loss = 19.7989 = -0.3058 + 0.5 * 40.2095 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 274 Reward: -1479.3009 total_loss = 19.7015 = -0.3000 + 0.5 * 40.0031 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 275 Reward: -1437.7748 total_loss = 19.3228 = -0.3189 + 0.5 * 39.2834 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 276 Reward: -1456.7015 total_loss = 19.7988 = -0.3294 + 0.5 * 40.2565 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 277 Reward: -1471.7599 total_loss = 18.1275 = -0.2890 + 0.5 * 36.8329 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 278 Reward: -1448.1174 total_loss = 21.4506 = -0.3059 + 0.5 * 43.5103 + 0.01 * 0.1377\n",
      "-----------------\n",
      "Finished episode: 279 Reward: -1493.3555 total_loss = 19.4987 = -0.2819 + 0.5 * 39.5611 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 280 Reward: -1348.1537 total_loss = 17.5217 = -0.2929 + 0.5 * 35.6240 + 0.01 * 0.2562\n",
      "-----------------\n",
      "Finished episode: 281 Reward: -1410.4591 total_loss = 18.5256 = -0.3143 + 0.5 * 37.6783 + 0.01 * 0.0751\n",
      "-----------------\n",
      "Finished episode: 282 Reward: -1413.7089 total_loss = 17.9800 = -0.2896 + 0.5 * 36.5356 + 0.01 * 0.1856\n",
      "-----------------\n",
      "Finished episode: 283 Reward: -1421.3002 total_loss = 19.6682 = -0.3131 + 0.5 * 39.9545 + 0.01 * 0.4099\n",
      "-----------------\n",
      "Finished episode: 284 Reward: -1508.6704 total_loss = 21.5214 = -0.2965 + 0.5 * 43.6357 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 285 Reward: -1439.8711 total_loss = 21.0147 = -0.3037 + 0.5 * 42.6357 + 0.01 * 0.0533\n",
      "-----------------\n",
      "Finished episode: 286 Reward: -1517.6590 total_loss = 20.0904 = -0.3019 + 0.5 * 40.7785 + 0.01 * 0.3082\n",
      "-----------------\n",
      "Finished episode: 287 Reward: -1481.2280 total_loss = 19.5424 = -0.3121 + 0.5 * 39.7090 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 288 Reward: -1430.8617 total_loss = 19.0543 = -0.2875 + 0.5 * 38.6754 + 0.01 * 0.4100\n",
      "-----------------\n",
      "Finished episode: 289 Reward: -1408.2774 total_loss = 18.9374 = -0.2825 + 0.5 * 38.4363 + 0.01 * 0.1784\n",
      "-----------------\n",
      "Finished episode: 290 Reward: -1429.3518 total_loss = 18.4768 = -0.2843 + 0.5 * 37.5221 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 291 Reward: -1433.1193 total_loss = 19.7462 = -0.2886 + 0.5 * 40.0696 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 292 Reward: -1418.2071 total_loss = 20.1357 = -0.2806 + 0.5 * 40.8326 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 293 Reward: -1375.5420 total_loss = 18.3632 = -0.2500 + 0.5 * 37.2231 + 0.01 * 0.1695\n",
      "-----------------\n",
      "Finished episode: 294 Reward: -1405.1428 total_loss = 20.4462 = -0.3032 + 0.5 * 41.4987 + 0.01 * -0.0006\n",
      "-----------------\n",
      "Finished episode: 295 Reward: -1463.2528 total_loss = 19.5092 = -0.3097 + 0.5 * 39.6378 + 0.01 * 0.0045\n",
      "-----------------\n",
      "Finished episode: 296 Reward: -1467.7310 total_loss = 20.0562 = -0.2928 + 0.5 * 40.6983 + 0.01 * -0.0113\n",
      "-----------------\n",
      "Finished episode: 297 Reward: -1445.5295 total_loss = 18.7024 = -0.2908 + 0.5 * 37.9863 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 298 Reward: -1470.3843 total_loss = 19.8108 = -0.3030 + 0.5 * 40.2277 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 299 Reward: -1411.0460 total_loss = 19.7347 = -0.3273 + 0.5 * 40.1239 + 0.01 * -0.0010\n",
      "-----------------\n",
      "Finished episode: 300 Reward: -1383.3047 total_loss = 17.1858 = -0.2795 + 0.5 * 34.9282 + 0.01 * 0.1228\n",
      "-----------------\n",
      "Finished episode: 301 Reward: -1342.6336 total_loss = 17.3616 = -0.2618 + 0.5 * 35.2468 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 302 Reward: -1453.4942 total_loss = 19.6759 = -0.3154 + 0.5 * 39.9793 + 0.01 * 0.1615\n",
      "-----------------\n",
      "Finished episode: 303 Reward: -1433.4413 total_loss = 20.2355 = -0.2971 + 0.5 * 41.0651 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 304 Reward: -1406.8004 total_loss = 18.9679 = -0.2988 + 0.5 * 38.5333 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 305 Reward: -1440.2732 total_loss = 20.2380 = -0.2921 + 0.5 * 41.0602 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 306 Reward: -1429.3927 total_loss = 19.2927 = -0.2933 + 0.5 * 39.1720 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 307 Reward: -1420.5772 total_loss = 18.8467 = -0.2951 + 0.5 * 38.2779 + 0.01 * 0.2906\n",
      "-----------------\n",
      "Finished episode: 308 Reward: -1341.5051 total_loss = 17.5297 = -0.2849 + 0.5 * 35.6292 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 309 Reward: -1451.2403 total_loss = 21.2495 = -0.2779 + 0.5 * 43.0530 + 0.01 * 0.0927\n",
      "-----------------\n",
      "Finished episode: 310 Reward: -1406.4229 total_loss = 20.3530 = -0.2818 + 0.5 * 41.2686 + 0.01 * 0.0552\n",
      "-----------------\n",
      "Finished episode: 311 Reward: -1419.6097 total_loss = 20.5130 = -0.3192 + 0.5 * 41.6643 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 312 Reward: -1402.9885 total_loss = 18.5403 = -0.3191 + 0.5 * 37.7188 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 313 Reward: -1429.9331 total_loss = 18.6337 = -0.2934 + 0.5 * 37.8532 + 0.01 * 0.0547\n",
      "-----------------\n",
      "Finished episode: 314 Reward: -1447.0332 total_loss = 17.7482 = -0.3104 + 0.5 * 36.1171 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 315 Reward: -1393.6597 total_loss = 18.4204 = -0.2819 + 0.5 * 37.4047 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 316 Reward: -1465.5857 total_loss = 19.9660 = -0.3023 + 0.5 * 40.5366 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 317 Reward: -1424.9759 total_loss = 17.2475 = -0.2925 + 0.5 * 35.0800 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 318 Reward: -1405.1814 total_loss = 17.0753 = -0.3108 + 0.5 * 34.7721 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 319 Reward: -1438.7959 total_loss = 18.9021 = -0.2896 + 0.5 * 38.3792 + 0.01 * 0.2173\n",
      "-----------------\n",
      "Finished episode: 320 Reward: -1447.7535 total_loss = 19.0662 = -0.3154 + 0.5 * 38.7633 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 321 Reward: -1450.8305 total_loss = 20.3696 = -0.2915 + 0.5 * 41.3222 + 0.01 * -0.0083\n",
      "-----------------\n",
      "Finished episode: 322 Reward: -1362.5539 total_loss = 19.8552 = -0.2796 + 0.5 * 40.2636 + 0.01 * 0.3007\n",
      "-----------------\n",
      "Finished episode: 323 Reward: -1422.3260 total_loss = 21.8124 = -0.2891 + 0.5 * 44.2031 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 324 Reward: -1451.1218 total_loss = 17.8963 = -0.3269 + 0.5 * 36.4426 + 0.01 * 0.1875\n",
      "-----------------\n",
      "Finished episode: 325 Reward: -1414.8610 total_loss = 19.0536 = -0.3010 + 0.5 * 38.6911 + 0.01 * 0.9100\n",
      "-----------------\n",
      "Finished episode: 326 Reward: -1356.4594 total_loss = 19.4566 = -0.2596 + 0.5 * 39.4324 + 0.01 * -0.0048\n",
      "-----------------\n",
      "Finished episode: 327 Reward: -1416.6090 total_loss = 18.8715 = -0.3074 + 0.5 * 38.3578 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 328 Reward: -1405.5364 total_loss = 19.5670 = -0.2834 + 0.5 * 39.7007 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 329 Reward: -1411.2147 total_loss = 18.3719 = -0.3149 + 0.5 * 37.3736 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 330 Reward: -1479.5140 total_loss = 18.9088 = -0.3315 + 0.5 * 38.4680 + 0.01 * 0.6267\n",
      "-----------------\n",
      "Finished episode: 331 Reward: -1515.2218 total_loss = 18.9691 = -0.4918 + 0.5 * 38.9132 + 0.01 * 0.4303\n",
      "-----------------\n",
      "Finished episode: 332 Reward: -1438.5317 total_loss = 19.4160 = -0.2788 + 0.5 * 39.3898 + 0.01 * -0.0049\n",
      "-----------------\n",
      "Finished episode: 333 Reward: -1417.8405 total_loss = 19.3373 = -0.3261 + 0.5 * 39.3072 + 0.01 * 0.9718\n",
      "-----------------\n",
      "Finished episode: 334 Reward: -1411.8165 total_loss = 18.5272 = -0.3097 + 0.5 * 37.6636 + 0.01 * 0.5148\n",
      "-----------------\n",
      "Finished episode: 335 Reward: -1472.1346 total_loss = 20.7859 = -0.2757 + 0.5 * 42.1207 + 0.01 * 0.1218\n",
      "-----------------\n",
      "Finished episode: 336 Reward: -1394.2633 total_loss = 20.5784 = -0.2984 + 0.5 * 41.7509 + 0.01 * 0.1357\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 337 Reward: -1361.6465 total_loss = 17.9754 = -0.2972 + 0.5 * 36.5346 + 0.01 * 0.5335\n",
      "-----------------\n",
      "Finished episode: 338 Reward: -1442.9846 total_loss = 20.2432 = -0.2759 + 0.5 * 41.0381 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 339 Reward: -1408.1472 total_loss = 15.7187 = -0.3115 + 0.5 * 32.0574 + 0.01 * 0.1488\n",
      "-----------------\n",
      "Finished episode: 340 Reward: -1442.7708 total_loss = 19.4498 = -0.2789 + 0.5 * 39.4573 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 341 Reward: -1475.2489 total_loss = 19.7954 = -0.2764 + 0.5 * 40.1437 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 342 Reward: -1423.5486 total_loss = 19.0127 = -0.2951 + 0.5 * 38.6156 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 343 Reward: -1488.8927 total_loss = 22.5846 = -0.3157 + 0.5 * 45.8006 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 344 Reward: -1413.2226 total_loss = 19.0819 = -0.3061 + 0.5 * 38.7761 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 345 Reward: -1397.2159 total_loss = 17.1082 = -0.2699 + 0.5 * 34.7562 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 346 Reward: -1358.4687 total_loss = 18.0067 = -0.3268 + 0.5 * 36.6619 + 0.01 * 0.2619\n",
      "-----------------\n",
      "Finished episode: 347 Reward: -1417.9011 total_loss = 20.2806 = -0.3085 + 0.5 * 41.1781 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 348 Reward: -1509.8104 total_loss = 20.6991 = -0.3306 + 0.5 * 42.0593 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 349 Reward: -1477.3504 total_loss = 17.5979 = -0.3077 + 0.5 * 35.8113 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 350 Reward: -1428.0449 total_loss = 19.9273 = -0.3051 + 0.5 * 40.4649 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 351 Reward: -1416.0945 total_loss = 18.2388 = -0.3202 + 0.5 * 37.1160 + 0.01 * 0.1012\n",
      "-----------------\n",
      "Finished episode: 352 Reward: -1458.9927 total_loss = 20.5578 = -0.2998 + 0.5 * 41.7086 + 0.01 * 0.3278\n",
      "-----------------\n",
      "Finished episode: 353 Reward: -1440.1596 total_loss = 18.8825 = -0.2826 + 0.5 * 38.3302 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 354 Reward: -1420.8817 total_loss = 19.3451 = -0.2919 + 0.5 * 39.2739 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 355 Reward: -1430.3849 total_loss = 18.4705 = -0.3005 + 0.5 * 37.5419 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 356 Reward: -1385.3376 total_loss = 17.3029 = -0.3213 + 0.5 * 35.2424 + 0.01 * 0.2899\n",
      "-----------------\n",
      "Finished episode: 357 Reward: -1397.8950 total_loss = 17.7162 = -0.2808 + 0.5 * 35.9940 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 358 Reward: -1471.1769 total_loss = 19.5435 = -0.3165 + 0.5 * 39.7199 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 359 Reward: -1350.4260 total_loss = 18.6129 = -0.3053 + 0.5 * 37.8365 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 360 Reward: -1407.6452 total_loss = 19.4630 = -0.2985 + 0.5 * 39.5230 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 361 Reward: -1429.3915 total_loss = 18.4524 = -0.3177 + 0.5 * 37.5402 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 362 Reward: -1390.2239 total_loss = 19.0474 = -0.3379 + 0.5 * 38.7691 + 0.01 * 0.0776\n",
      "-----------------\n",
      "Finished episode: 363 Reward: -1448.3215 total_loss = 18.7646 = -0.2847 + 0.5 * 38.0986 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 364 Reward: -1446.6611 total_loss = 18.0838 = -0.3113 + 0.5 * 36.7902 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 365 Reward: -1470.0405 total_loss = 19.3112 = -0.2861 + 0.5 * 39.1947 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 366 Reward: -1482.9296 total_loss = 19.4780 = -0.3056 + 0.5 * 39.5655 + 0.01 * 0.0767\n",
      "-----------------\n",
      "Finished episode: 367 Reward: -1353.6093 total_loss = 17.3381 = -0.2960 + 0.5 * 35.2682 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 368 Reward: -1449.1808 total_loss = 17.9620 = -0.2867 + 0.5 * 36.4975 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 369 Reward: -1443.9555 total_loss = 18.8314 = -0.2952 + 0.5 * 38.2462 + 0.01 * 0.3499\n",
      "-----------------\n",
      "Finished episode: 370 Reward: -1477.7039 total_loss = 19.5787 = -0.2883 + 0.5 * 39.7340 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 371 Reward: -1468.9120 total_loss = 20.6592 = -0.3149 + 0.5 * 41.9482 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 372 Reward: -1357.3138 total_loss = 17.8659 = -0.3230 + 0.5 * 36.3777 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 373 Reward: -1439.1188 total_loss = 21.0758 = -0.2876 + 0.5 * 42.7267 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 374 Reward: -1415.1051 total_loss = 17.8956 = -0.2959 + 0.5 * 36.3816 + 0.01 * 0.0757\n",
      "-----------------\n",
      "Finished episode: 375 Reward: -1406.3242 total_loss = 17.8528 = -0.2889 + 0.5 * 36.2835 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 376 Reward: -1375.8740 total_loss = 17.4069 = -0.3103 + 0.5 * 35.4343 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 377 Reward: -1399.2994 total_loss = 18.8075 = -0.3123 + 0.5 * 38.2354 + 0.01 * 0.2147\n",
      "-----------------\n",
      "Finished episode: 378 Reward: -1489.2760 total_loss = 21.6552 = -0.2875 + 0.5 * 43.8834 + 0.01 * 0.0908\n",
      "-----------------\n",
      "Finished episode: 379 Reward: -1498.1152 total_loss = 19.7980 = -0.2828 + 0.5 * 40.1569 + 0.01 * 0.2344\n",
      "-----------------\n",
      "Finished episode: 380 Reward: -1424.1390 total_loss = 20.3790 = -0.2769 + 0.5 * 41.3118 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 381 Reward: -1404.1698 total_loss = 19.2827 = -0.3014 + 0.5 * 39.1681 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 382 Reward: -1481.7811 total_loss = 20.7350 = -0.3240 + 0.5 * 42.1181 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 383 Reward: -1453.8577 total_loss = 20.3247 = -0.3005 + 0.5 * 41.2503 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 384 Reward: -1425.5764 total_loss = 19.2906 = -0.2667 + 0.5 * 39.1105 + 0.01 * 0.2010\n",
      "-----------------\n",
      "Finished episode: 385 Reward: -1406.1992 total_loss = 19.0279 = -0.2935 + 0.5 * 38.6428 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 386 Reward: -1418.0430 total_loss = 20.1831 = -0.3218 + 0.5 * 41.0016 + 0.01 * 0.4079\n",
      "-----------------\n",
      "Finished episode: 387 Reward: -1423.5708 total_loss = 19.1005 = -0.3089 + 0.5 * 38.8187 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 388 Reward: -1445.0605 total_loss = 19.9545 = -0.3105 + 0.5 * 40.5251 + 0.01 * 0.2394\n",
      "-----------------\n",
      "Finished episode: 389 Reward: -1447.4564 total_loss = 19.3617 = -0.3198 + 0.5 * 39.3630 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 390 Reward: -1353.5967 total_loss = 18.8257 = -0.3792 + 0.5 * 38.3981 + 0.01 * 0.5840\n",
      "-----------------\n",
      "Finished episode: 391 Reward: -1460.6793 total_loss = 19.8740 = -0.3166 + 0.5 * 40.3812 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 392 Reward: -1485.6413 total_loss = 21.1816 = -0.3122 + 0.5 * 42.9737 + 0.01 * 0.6953\n",
      "-----------------\n",
      "Finished episode: 393 Reward: -1427.3217 total_loss = 20.7819 = -0.3147 + 0.5 * 42.1931 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 394 Reward: -1406.0148 total_loss = 17.6477 = -0.2962 + 0.5 * 35.8801 + 0.01 * 0.3851\n",
      "-----------------\n",
      "Finished episode: 395 Reward: -1470.3487 total_loss = 21.9509 = -0.3011 + 0.5 * 44.5039 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 396 Reward: -1389.8872 total_loss = 19.3510 = -0.3133 + 0.5 * 39.3185 + 0.01 * 0.5031\n",
      "-----------------\n",
      "Finished episode: 397 Reward: -1458.2999 total_loss = 21.1562 = -0.3020 + 0.5 * 42.9165 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 398 Reward: -1494.8825 total_loss = 20.2922 = -0.3005 + 0.5 * 41.1854 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 399 Reward: -1395.1601 total_loss = 20.0923 = -0.2761 + 0.5 * 40.7171 + 0.01 * 0.9779\n",
      "-----------------\n",
      "Finished episode: 400 Reward: -1422.6565 total_loss = 19.9153 = -0.2861 + 0.5 * 40.3818 + 0.01 * 1.0473\n",
      "-----------------\n",
      "Finished episode: 401 Reward: -1426.5762 total_loss = 19.5267 = -0.3278 + 0.5 * 39.6963 + 0.01 * 0.6326\n",
      "-----------------\n",
      "Finished episode: 402 Reward: -1476.4770 total_loss = 21.0090 = -0.3094 + 0.5 * 42.6159 + 0.01 * 1.0497\n",
      "-----------------\n",
      "Finished episode: 403 Reward: -1414.3595 total_loss = 19.7686 = -0.3635 + 0.5 * 40.2501 + 0.01 * 0.7113\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 404 Reward: -1397.2271 total_loss = 17.8059 = -0.3214 + 0.5 * 36.2278 + 0.01 * 1.3441\n",
      "-----------------\n",
      "Finished episode: 405 Reward: -1478.2711 total_loss = 20.5978 = -0.3141 + 0.5 * 41.8219 + 0.01 * 0.0965\n",
      "-----------------\n",
      "Finished episode: 406 Reward: -1401.7871 total_loss = 18.9366 = -0.2727 + 0.5 * 38.3951 + 0.01 * 1.1793\n",
      "-----------------\n",
      "Finished episode: 407 Reward: -1394.2973 total_loss = 19.9795 = -0.3113 + 0.5 * 40.5719 + 0.01 * 0.4751\n",
      "-----------------\n",
      "Finished episode: 408 Reward: -1459.9675 total_loss = 19.7157 = -0.3435 + 0.5 * 40.1142 + 0.01 * 0.2127\n",
      "-----------------\n",
      "Finished episode: 409 Reward: -1488.4421 total_loss = 21.7583 = -0.2817 + 0.5 * 44.0801 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 410 Reward: -1393.6482 total_loss = 18.4028 = -0.2880 + 0.5 * 37.3817 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 411 Reward: -1446.8386 total_loss = 21.5386 = -0.3022 + 0.5 * 43.6383 + 0.01 * 2.1698\n",
      "-----------------\n",
      "Finished episode: 412 Reward: -1497.8011 total_loss = 19.7587 = -0.5983 + 0.5 * 40.7032 + 0.01 * 0.5402\n",
      "-----------------\n",
      "Finished episode: 413 Reward: -1487.5017 total_loss = 20.3783 = -0.3022 + 0.5 * 41.3561 + 0.01 * 0.2524\n",
      "-----------------\n",
      "Finished episode: 414 Reward: -1431.9437 total_loss = 19.4870 = -0.2530 + 0.5 * 39.4760 + 0.01 * 0.2040\n",
      "-----------------\n",
      "Finished episode: 415 Reward: -1436.7551 total_loss = 20.6262 = -0.4940 + 0.5 * 42.2095 + 0.01 * 1.5390\n",
      "-----------------\n",
      "Finished episode: 416 Reward: -1411.2767 total_loss = 20.5748 = -0.3515 + 0.5 * 41.8411 + 0.01 * 0.5678\n",
      "-----------------\n",
      "Finished episode: 417 Reward: -1512.8422 total_loss = 21.3029 = -0.2899 + 0.5 * 43.1417 + 0.01 * 2.1937\n",
      "-----------------\n",
      "Finished episode: 418 Reward: -1513.9202 total_loss = 19.9767 = -0.3085 + 0.5 * 40.5594 + 0.01 * 0.5440\n",
      "-----------------\n",
      "Finished episode: 419 Reward: -1536.4155 total_loss = 20.4275 = -0.4450 + 0.5 * 41.7383 + 0.01 * 0.3354\n",
      "-----------------\n",
      "Finished episode: 420 Reward: -1517.7956 total_loss = 17.3345 = -0.3086 + 0.5 * 35.2771 + 0.01 * 0.4581\n",
      "-----------------\n",
      "Finished episode: 421 Reward: -1497.6878 total_loss = 22.9179 = -0.2836 + 0.5 * 46.4030 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 422 Reward: -1498.9306 total_loss = 18.4095 = -0.3203 + 0.5 * 37.4595 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 423 Reward: -1468.3733 total_loss = 20.6280 = -0.2783 + 0.5 * 41.7997 + 0.01 * 0.6473\n",
      "-----------------\n",
      "Finished episode: 424 Reward: -1542.5443 total_loss = 19.7730 = -0.3198 + 0.5 * 40.1823 + 0.01 * 0.1648\n",
      "-----------------\n",
      "Finished episode: 425 Reward: -1519.7540 total_loss = 19.8573 = -0.3390 + 0.5 * 40.3927 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 426 Reward: -1489.4947 total_loss = 21.7271 = -0.2893 + 0.5 * 44.0328 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 427 Reward: -1484.5423 total_loss = 21.8290 = -0.3284 + 0.5 * 44.3148 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 428 Reward: -1492.2778 total_loss = 18.9888 = -0.3283 + 0.5 * 38.6342 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 429 Reward: -1473.0108 total_loss = 19.8125 = -0.3208 + 0.5 * 40.2602 + 0.01 * 0.3177\n",
      "-----------------\n",
      "Finished episode: 430 Reward: -1508.1679 total_loss = 19.2028 = -0.3340 + 0.5 * 39.0738 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 431 Reward: -1461.1507 total_loss = 21.8943 = -0.3657 + 0.5 * 44.4953 + 0.01 * 1.2323\n",
      "-----------------\n",
      "Finished episode: 432 Reward: -1491.9527 total_loss = 22.2339 = -0.3167 + 0.5 * 45.1012 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 433 Reward: -1498.4547 total_loss = 21.0288 = -0.2783 + 0.5 * 42.6143 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 434 Reward: -1398.6829 total_loss = 20.5981 = -0.2857 + 0.5 * 41.7331 + 0.01 * 1.7256\n",
      "-----------------\n",
      "Finished episode: 435 Reward: -1514.3196 total_loss = 20.2179 = -0.3129 + 0.5 * 41.0616 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 436 Reward: -1440.6672 total_loss = 19.5222 = -0.2863 + 0.5 * 39.6171 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 437 Reward: -1370.5706 total_loss = 19.8398 = -0.2887 + 0.5 * 40.2571 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 438 Reward: -1473.4634 total_loss = 21.3817 = -0.2948 + 0.5 * 43.3302 + 0.01 * 1.1319\n",
      "-----------------\n",
      "Finished episode: 439 Reward: -1469.7657 total_loss = 20.7351 = -0.3060 + 0.5 * 42.0821 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 440 Reward: -1492.3898 total_loss = 22.5282 = -0.2789 + 0.5 * 45.6144 + 0.01 * -0.0019\n",
      "-----------------\n",
      "Finished episode: 441 Reward: -1500.0730 total_loss = 20.8945 = -0.3341 + 0.5 * 42.4485 + 0.01 * 0.4373\n",
      "-----------------\n",
      "Finished episode: 442 Reward: -1431.8125 total_loss = 18.4447 = -0.2981 + 0.5 * 37.4856 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 443 Reward: -1502.5652 total_loss = 21.7242 = -0.2745 + 0.5 * 43.9974 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 444 Reward: -1392.0415 total_loss = 18.4810 = -0.2958 + 0.5 * 37.5537 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 445 Reward: -1463.2540 total_loss = 21.7264 = -0.2960 + 0.5 * 44.0449 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 446 Reward: -1417.6426 total_loss = 20.1823 = -0.3209 + 0.5 * 41.0064 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 447 Reward: -1492.0291 total_loss = 21.3330 = -0.3107 + 0.5 * 43.2874 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 448 Reward: -1421.8243 total_loss = 17.6421 = -0.2658 + 0.5 * 35.8100 + 0.01 * 0.2899\n",
      "-----------------\n",
      "Finished episode: 449 Reward: -1481.0988 total_loss = 20.9809 = -0.2987 + 0.5 * 42.5591 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 450 Reward: -1450.5599 total_loss = 21.5187 = -0.3164 + 0.5 * 43.6702 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 451 Reward: -1435.4573 total_loss = 20.4691 = -0.2765 + 0.5 * 41.4912 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 452 Reward: -1388.1276 total_loss = 19.2577 = -0.2834 + 0.5 * 39.0744 + 0.01 * 0.3824\n",
      "-----------------\n",
      "Finished episode: 453 Reward: -1361.5652 total_loss = 18.2166 = -0.2769 + 0.5 * 36.9870 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 454 Reward: -1412.4378 total_loss = 19.9549 = -0.2822 + 0.5 * 40.4743 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 455 Reward: -1474.3225 total_loss = 19.0502 = -0.2983 + 0.5 * 38.6970 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 456 Reward: -1387.1449 total_loss = 19.1320 = -0.2747 + 0.5 * 38.8133 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 457 Reward: -1357.3939 total_loss = 18.5445 = -0.3087 + 0.5 * 37.7063 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 458 Reward: -1448.2507 total_loss = 19.0049 = -0.2913 + 0.5 * 38.5924 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 459 Reward: -1450.0059 total_loss = 19.8171 = -0.2899 + 0.5 * 40.2140 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 460 Reward: -1394.7206 total_loss = 18.1074 = -0.2908 + 0.5 * 36.7965 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 461 Reward: -1411.5584 total_loss = 18.4230 = -0.2623 + 0.5 * 37.3707 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 462 Reward: -1440.0223 total_loss = 18.9450 = -0.2923 + 0.5 * 38.4747 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 463 Reward: -1415.2467 total_loss = 18.3601 = -0.3059 + 0.5 * 37.3319 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 464 Reward: -1443.4555 total_loss = 20.5601 = -0.2829 + 0.5 * 41.6820 + 0.01 * 0.1975\n",
      "-----------------\n",
      "Finished episode: 465 Reward: -1443.8247 total_loss = 18.6321 = -0.3033 + 0.5 * 37.8709 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 466 Reward: -1413.7539 total_loss = 20.5134 = -0.2929 + 0.5 * 41.6075 + 0.01 * 0.2562\n",
      "-----------------\n",
      "Finished episode: 467 Reward: -1338.1431 total_loss = 18.4703 = -0.3157 + 0.5 * 37.5720 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 468 Reward: -1415.8943 total_loss = 19.2602 = -0.3000 + 0.5 * 39.1204 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 469 Reward: -1384.3057 total_loss = 18.8737 = -0.2988 + 0.5 * 38.3449 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 470 Reward: -1398.6474 total_loss = 18.5192 = -0.3084 + 0.5 * 37.6553 + 0.01 * 0.0000\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 471 Reward: -1359.1580 total_loss = 17.5813 = -0.3077 + 0.5 * 35.7780 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 472 Reward: -1510.9126 total_loss = 18.9423 = -0.3007 + 0.5 * 38.4860 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 473 Reward: -1470.7318 total_loss = 19.8098 = -0.3087 + 0.5 * 40.2371 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 474 Reward: -1376.5026 total_loss = 17.1442 = -0.3010 + 0.5 * 34.8841 + 0.01 * 0.3082\n",
      "-----------------\n",
      "Finished episode: 475 Reward: -1384.2734 total_loss = 18.2684 = -0.2895 + 0.5 * 37.1158 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 476 Reward: -1485.2423 total_loss = 21.5180 = -0.3053 + 0.5 * 43.6465 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 477 Reward: -1375.2573 total_loss = 19.5254 = -0.3022 + 0.5 * 39.6513 + 0.01 * 0.1910\n",
      "-----------------\n",
      "Finished episode: 478 Reward: -1434.8974 total_loss = 18.2033 = -0.2918 + 0.5 * 36.9818 + 0.01 * 0.4165\n",
      "-----------------\n",
      "Finished episode: 479 Reward: -1425.9695 total_loss = 18.4189 = -0.2946 + 0.5 * 37.4269 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 480 Reward: -1401.5078 total_loss = 20.5865 = -0.3285 + 0.5 * 41.8299 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 481 Reward: -1326.7483 total_loss = 17.3531 = -0.2975 + 0.5 * 35.3012 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 482 Reward: -1419.0940 total_loss = 20.4578 = -0.2915 + 0.5 * 41.4986 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 483 Reward: -1455.8886 total_loss = 19.7598 = -0.3119 + 0.5 * 40.1434 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 484 Reward: -1414.6207 total_loss = 19.7310 = -0.2492 + 0.5 * 39.9605 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 485 Reward: -1484.6716 total_loss = 19.4261 = -0.3225 + 0.5 * 39.4972 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 486 Reward: -1444.0199 total_loss = 19.3317 = -0.3066 + 0.5 * 39.2766 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 487 Reward: -1426.9370 total_loss = 20.7626 = -0.2922 + 0.5 * 42.1095 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 488 Reward: -1450.2632 total_loss = 19.7274 = -0.3048 + 0.5 * 40.0643 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 489 Reward: -1471.7339 total_loss = 21.5054 = -0.3122 + 0.5 * 43.6354 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 490 Reward: -1460.8018 total_loss = 20.3839 = -0.3123 + 0.5 * 41.3825 + 0.01 * 0.4987\n",
      "-----------------\n",
      "Finished episode: 491 Reward: -1335.7211 total_loss = 18.4362 = -0.3126 + 0.5 * 37.4976 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 492 Reward: -1468.0717 total_loss = 19.2153 = -0.3356 + 0.5 * 39.1000 + 0.01 * 0.0984\n",
      "-----------------\n",
      "Finished episode: 493 Reward: -1457.0698 total_loss = 20.5442 = -0.2894 + 0.5 * 41.6672 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 494 Reward: -1385.5228 total_loss = 20.2904 = -0.2992 + 0.5 * 41.1700 + 0.01 * 0.4627\n",
      "-----------------\n",
      "Finished episode: 495 Reward: -1424.6176 total_loss = 19.9167 = -0.3102 + 0.5 * 40.4537 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 496 Reward: -1451.1710 total_loss = 20.2408 = -0.3136 + 0.5 * 41.1089 + 0.01 * -0.0006\n",
      "-----------------\n",
      "Finished episode: 497 Reward: -1457.4262 total_loss = 20.2718 = -0.2743 + 0.5 * 41.0921 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 498 Reward: -1442.5106 total_loss = 18.7193 = -0.3148 + 0.5 * 38.0683 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 499 Reward: -1463.9386 total_loss = 19.5591 = -0.2827 + 0.5 * 39.6772 + 0.01 * 0.3184\n",
      "-----------------\n",
      "Finished episode: 500 Reward: -1436.4952 total_loss = 20.4960 = -0.2997 + 0.5 * 41.5915 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 501 Reward: -1433.9607 total_loss = 18.6101 = -0.3240 + 0.5 * 37.8612 + 0.01 * 0.3493\n",
      "-----------------\n",
      "Finished episode: 502 Reward: -1429.8509 total_loss = 20.4962 = -0.2972 + 0.5 * 41.5793 + 0.01 * 0.3744\n",
      "-----------------\n",
      "Finished episode: 503 Reward: -1419.1828 total_loss = 19.9665 = -0.2871 + 0.5 * 40.5072 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 504 Reward: -1408.8076 total_loss = 19.3987 = -0.3138 + 0.5 * 39.4249 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 505 Reward: -1387.4335 total_loss = 17.9101 = -0.3405 + 0.5 * 36.4921 + 0.01 * 0.4490\n",
      "-----------------\n",
      "Finished episode: 506 Reward: -1435.1934 total_loss = 19.7862 = -0.3287 + 0.5 * 40.2299 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 507 Reward: -1494.8081 total_loss = 22.1278 = -0.2838 + 0.5 * 44.8231 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 508 Reward: -1441.3303 total_loss = 20.8373 = -0.3337 + 0.5 * 42.3342 + 0.01 * 0.3944\n",
      "-----------------\n",
      "Finished episode: 509 Reward: -1534.7991 total_loss = 21.4711 = -0.2938 + 0.5 * 43.5298 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 510 Reward: -1507.4713 total_loss = 20.4054 = -0.3021 + 0.5 * 41.4151 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 511 Reward: -1410.7119 total_loss = 19.4895 = -0.3019 + 0.5 * 39.5827 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 512 Reward: -1499.2820 total_loss = 22.4881 = -0.2685 + 0.5 * 45.5060 + 0.01 * 0.3654\n",
      "-----------------\n",
      "Finished episode: 513 Reward: -1435.4020 total_loss = 21.2780 = -0.3186 + 0.5 * 43.1932 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 514 Reward: -1466.3467 total_loss = 19.9539 = -0.2990 + 0.5 * 40.4968 + 0.01 * 0.4499\n",
      "-----------------\n",
      "Finished episode: 515 Reward: -1387.8421 total_loss = 16.3252 = -0.3031 + 0.5 * 33.2567 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 516 Reward: -1496.1386 total_loss = 21.5229 = -0.3001 + 0.5 * 43.6458 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 517 Reward: -1432.4585 total_loss = 21.8565 = -0.2923 + 0.5 * 44.2976 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 518 Reward: -1441.8465 total_loss = 19.9901 = -0.2906 + 0.5 * 40.5614 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 519 Reward: -1353.4194 total_loss = 18.8073 = -0.3096 + 0.5 * 38.2338 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 520 Reward: -1480.6445 total_loss = 18.5413 = -0.2788 + 0.5 * 37.6402 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 521 Reward: -1428.9122 total_loss = 20.4606 = -0.2988 + 0.5 * 41.5124 + 0.01 * 0.3166\n",
      "-----------------\n",
      "Finished episode: 522 Reward: -1402.6106 total_loss = 20.0195 = -0.3308 + 0.5 * 40.6928 + 0.01 * 0.3979\n",
      "-----------------\n",
      "Finished episode: 523 Reward: -1432.2490 total_loss = 18.9298 = -0.3001 + 0.5 * 38.4594 + 0.01 * 0.0144\n",
      "-----------------\n",
      "Finished episode: 524 Reward: -1423.3396 total_loss = 19.7780 = -0.3130 + 0.5 * 40.1746 + 0.01 * 0.3641\n",
      "-----------------\n",
      "Finished episode: 525 Reward: -1434.6187 total_loss = 19.0088 = -0.2799 + 0.5 * 38.5774 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 526 Reward: -1431.4843 total_loss = 18.3720 = -0.3072 + 0.5 * 37.3585 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 527 Reward: -1488.3877 total_loss = 20.7711 = -0.2850 + 0.5 * 42.1121 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 528 Reward: -1351.8167 total_loss = 19.3941 = -0.3325 + 0.5 * 39.4531 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 529 Reward: -1432.1420 total_loss = 16.5849 = -0.3092 + 0.5 * 33.7881 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 530 Reward: -1456.5814 total_loss = 19.6720 = -0.3145 + 0.5 * 39.9695 + 0.01 * 0.1715\n",
      "-----------------\n",
      "Finished episode: 531 Reward: -1458.2195 total_loss = 18.6598 = -0.2952 + 0.5 * 37.9023 + 0.01 * 0.3859\n",
      "-----------------\n",
      "Finished episode: 532 Reward: -1502.6274 total_loss = 19.7572 = -0.3102 + 0.5 * 40.1348 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 533 Reward: -1477.5227 total_loss = 18.6967 = -0.3311 + 0.5 * 38.0557 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 534 Reward: -1434.5215 total_loss = 20.7867 = -0.2626 + 0.5 * 42.0889 + 0.01 * 0.4805\n",
      "-----------------\n",
      "Finished episode: 535 Reward: -1442.4032 total_loss = 19.1663 = -0.2976 + 0.5 * 38.9277 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 536 Reward: -1510.9263 total_loss = 20.4278 = -0.3061 + 0.5 * 41.4678 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 537 Reward: -1540.4706 total_loss = 20.0199 = -0.3231 + 0.5 * 40.6819 + 0.01 * 0.2048\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 538 Reward: -1561.1824 total_loss = 21.7402 = -0.3289 + 0.5 * 44.1327 + 0.01 * 0.2829\n",
      "-----------------\n",
      "Finished episode: 539 Reward: -1567.5721 total_loss = 21.0866 = -0.3222 + 0.5 * 42.8144 + 0.01 * 0.1537\n",
      "-----------------\n",
      "Finished episode: 540 Reward: -1568.2441 total_loss = 20.8436 = -0.3417 + 0.5 * 42.3598 + 0.01 * 0.5400\n",
      "-----------------\n",
      "Finished episode: 541 Reward: -1544.9485 total_loss = 20.2150 = -0.2974 + 0.5 * 41.0249 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 542 Reward: -1577.0382 total_loss = 19.4041 = -0.3079 + 0.5 * 39.4196 + 0.01 * 0.2280\n",
      "-----------------\n",
      "Finished episode: 543 Reward: -1542.4434 total_loss = 19.2062 = -0.3546 + 0.5 * 39.1168 + 0.01 * 0.2362\n",
      "-----------------\n",
      "Finished episode: 544 Reward: -1546.2023 total_loss = 20.2638 = -0.3172 + 0.5 * 41.1567 + 0.01 * 0.2681\n",
      "-----------------\n",
      "Finished episode: 545 Reward: -1577.5276 total_loss = 20.8998 = -0.3364 + 0.5 * 42.4725 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 546 Reward: -1537.8174 total_loss = 20.1022 = -0.3085 + 0.5 * 40.8201 + 0.01 * 0.0669\n",
      "-----------------\n",
      "Finished episode: 547 Reward: -1528.3301 total_loss = 19.6992 = -0.3397 + 0.5 * 40.0555 + 0.01 * 1.1235\n",
      "-----------------\n",
      "Finished episode: 548 Reward: -1545.9230 total_loss = 20.5778 = -0.3643 + 0.5 * 41.8842 + 0.01 * -0.0025\n",
      "-----------------\n",
      "Finished episode: 549 Reward: -1543.1644 total_loss = 17.9489 = -0.3091 + 0.5 * 36.5130 + 0.01 * 0.1480\n",
      "-----------------\n",
      "Finished episode: 550 Reward: -1530.9590 total_loss = 20.3684 = -0.3041 + 0.5 * 41.3440 + 0.01 * 0.0497\n",
      "-----------------\n",
      "Finished episode: 551 Reward: -1538.2245 total_loss = 18.7057 = -0.2849 + 0.5 * 37.9812 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 552 Reward: -1515.5564 total_loss = 19.7407 = -0.3305 + 0.5 * 40.1393 + 0.01 * 0.1491\n",
      "-----------------\n",
      "Finished episode: 553 Reward: -1512.8107 total_loss = 19.8791 = -0.3002 + 0.5 * 40.3496 + 0.01 * 0.4481\n",
      "-----------------\n",
      "Finished episode: 554 Reward: -1569.3281 total_loss = 20.5183 = -0.3113 + 0.5 * 41.6593 + 0.01 * -0.0005\n",
      "-----------------\n",
      "Finished episode: 555 Reward: -1545.3105 total_loss = 18.4624 = -0.3315 + 0.5 * 37.5880 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 556 Reward: -1528.3137 total_loss = 18.5774 = -0.3648 + 0.5 * 37.8701 + 0.01 * 0.7182\n",
      "-----------------\n",
      "Finished episode: 557 Reward: -1526.7284 total_loss = 19.8872 = -0.3458 + 0.5 * 40.4654 + 0.01 * 0.0306\n",
      "-----------------\n",
      "Finished episode: 558 Reward: -1554.7871 total_loss = 20.7853 = -0.3443 + 0.5 * 42.2593 + 0.01 * -0.0015\n",
      "-----------------\n",
      "Finished episode: 559 Reward: -1551.5386 total_loss = 20.3023 = -0.3258 + 0.5 * 41.2470 + 0.01 * 0.4569\n",
      "-----------------\n",
      "Finished episode: 560 Reward: -1574.6559 total_loss = 19.8414 = -0.3243 + 0.5 * 40.3315 + 0.01 * -0.0033\n",
      "-----------------\n",
      "Finished episode: 561 Reward: -1561.7235 total_loss = 20.6597 = -0.2850 + 0.5 * 41.8893 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 562 Reward: -1544.3548 total_loss = 20.5754 = -0.3284 + 0.5 * 41.7984 + 0.01 * 0.4653\n",
      "-----------------\n",
      "Finished episode: 563 Reward: -1533.4222 total_loss = 20.5016 = -0.3276 + 0.5 * 41.6550 + 0.01 * 0.1716\n",
      "-----------------\n",
      "Finished episode: 564 Reward: -1539.6773 total_loss = 20.8184 = -0.3212 + 0.5 * 42.2791 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 565 Reward: -1575.1261 total_loss = 22.2023 = -0.3125 + 0.5 * 45.0295 + 0.01 * 0.0094\n",
      "-----------------\n",
      "Finished episode: 566 Reward: -1531.7067 total_loss = 18.3301 = -0.3197 + 0.5 * 37.2996 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 567 Reward: -1554.0608 total_loss = 19.5684 = -0.3187 + 0.5 * 39.7741 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 568 Reward: -1551.5148 total_loss = 18.7469 = -0.3362 + 0.5 * 38.1663 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 569 Reward: -1557.0408 total_loss = 16.9094 = -0.3532 + 0.5 * 34.5182 + 0.01 * 0.3511\n",
      "-----------------\n",
      "Finished episode: 570 Reward: -1551.9432 total_loss = 18.7082 = -0.3113 + 0.5 * 38.0388 + 0.01 * 0.0102\n",
      "-----------------\n",
      "Finished episode: 571 Reward: -1546.9054 total_loss = 19.0787 = -0.3056 + 0.5 * 38.7686 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 572 Reward: -1537.7075 total_loss = 17.8605 = -0.3292 + 0.5 * 36.3756 + 0.01 * 0.1876\n",
      "-----------------\n",
      "Finished episode: 573 Reward: -1554.1368 total_loss = 20.6312 = -0.3128 + 0.5 * 41.8879 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 574 Reward: -1535.4214 total_loss = 19.3353 = -0.2778 + 0.5 * 39.2261 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 575 Reward: -1570.6739 total_loss = 20.6043 = -0.2802 + 0.5 * 41.7689 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 576 Reward: -1573.3686 total_loss = 19.9754 = -0.3083 + 0.5 * 40.5673 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 577 Reward: -1553.8396 total_loss = 21.5683 = -0.3362 + 0.5 * 43.8090 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 578 Reward: -1593.1055 total_loss = 20.8991 = -0.3442 + 0.5 * 42.4843 + 0.01 * 0.1157\n",
      "-----------------\n",
      "Finished episode: 579 Reward: -1593.7991 total_loss = 20.0678 = -0.3131 + 0.5 * 40.7578 + 0.01 * 0.1967\n",
      "-----------------\n",
      "Finished episode: 580 Reward: -1581.4632 total_loss = 19.3246 = -0.3039 + 0.5 * 39.2571 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 581 Reward: -1588.9486 total_loss = 20.4649 = -0.3401 + 0.5 * 41.6082 + 0.01 * 0.0899\n",
      "-----------------\n",
      "Finished episode: 582 Reward: -1571.1590 total_loss = 20.9478 = -0.2869 + 0.5 * 42.4693 + 0.01 * 0.0058\n",
      "-----------------\n",
      "Finished episode: 583 Reward: -1575.5311 total_loss = 20.4090 = -0.3303 + 0.5 * 41.4767 + 0.01 * 0.0959\n",
      "-----------------\n",
      "Finished episode: 584 Reward: -1593.6426 total_loss = 20.4987 = -0.3352 + 0.5 * 41.6677 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 585 Reward: -1610.2535 total_loss = 20.7699 = -0.2908 + 0.5 * 42.1214 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 586 Reward: -1566.9151 total_loss = 22.4248 = -0.2939 + 0.5 * 45.4349 + 0.01 * 0.1273\n",
      "-----------------\n",
      "Finished episode: 587 Reward: -1587.6058 total_loss = 20.7734 = -0.3068 + 0.5 * 42.1603 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 588 Reward: -1603.4028 total_loss = 20.1761 = -0.3414 + 0.5 * 41.0249 + 0.01 * 0.4945\n",
      "-----------------\n",
      "Finished episode: 589 Reward: -1583.5650 total_loss = 20.1690 = -0.3090 + 0.5 * 40.9560 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 590 Reward: -1585.4220 total_loss = 20.6520 = -0.3061 + 0.5 * 41.9162 + 0.01 * -0.0017\n",
      "-----------------\n",
      "Finished episode: 591 Reward: -1607.0337 total_loss = 20.5646 = -0.2896 + 0.5 * 41.7061 + 0.01 * 0.1156\n",
      "-----------------\n",
      "Finished episode: 592 Reward: -1573.9200 total_loss = 18.6890 = -0.3122 + 0.5 * 38.0005 + 0.01 * 0.0932\n",
      "-----------------\n",
      "Finished episode: 593 Reward: -1585.3336 total_loss = 20.3674 = -0.3301 + 0.5 * 41.3950 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 594 Reward: -1586.5660 total_loss = 21.3732 = -0.2935 + 0.5 * 43.3334 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 595 Reward: -1572.3243 total_loss = 21.1382 = -0.2895 + 0.5 * 42.8554 + 0.01 * -0.0034\n",
      "-----------------\n",
      "Finished episode: 596 Reward: -1581.6707 total_loss = 20.0000 = -0.3244 + 0.5 * 40.6487 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 597 Reward: -1576.1093 total_loss = 18.5339 = -0.3108 + 0.5 * 37.6893 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 598 Reward: -1570.2272 total_loss = 19.8584 = -0.2983 + 0.5 * 40.3134 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 599 Reward: -1583.7000 total_loss = 20.6218 = -0.2914 + 0.5 * 41.8265 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 600 Reward: -1580.5670 total_loss = 21.7696 = -0.3309 + 0.5 * 44.2010 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 601 Reward: -1569.5534 total_loss = 20.0107 = -0.3018 + 0.5 * 40.6252 + 0.01 * -0.0017\n",
      "-----------------\n",
      "Finished episode: 602 Reward: -1571.5830 total_loss = 19.5699 = -0.3232 + 0.5 * 39.7830 + 0.01 * 0.1549\n",
      "-----------------\n",
      "Finished episode: 603 Reward: -1585.4099 total_loss = 19.3092 = -0.2761 + 0.5 * 39.1706 + 0.01 * -0.0016\n",
      "-----------------\n",
      "Finished episode: 604 Reward: -1610.6307 total_loss = 20.6580 = -0.3134 + 0.5 * 41.9412 + 0.01 * 0.0789\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 605 Reward: -1587.7331 total_loss = 20.6059 = -0.3380 + 0.5 * 41.8877 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 606 Reward: -1597.0890 total_loss = 19.2620 = -0.3001 + 0.5 * 39.1242 + 0.01 * 0.0006\n",
      "-----------------\n",
      "Finished episode: 607 Reward: -1597.1468 total_loss = 18.7220 = -0.3125 + 0.5 * 38.0690 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 608 Reward: -1601.3834 total_loss = 19.3770 = -0.3374 + 0.5 * 39.4247 + 0.01 * 0.1967\n",
      "-----------------\n",
      "Finished episode: 609 Reward: -1595.9657 total_loss = 20.7104 = -0.3159 + 0.5 * 42.0526 + 0.01 * 0.0027\n",
      "-----------------\n",
      "Finished episode: 610 Reward: -1586.8859 total_loss = 20.0306 = -0.3216 + 0.5 * 40.7044 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 611 Reward: -1579.7790 total_loss = 21.3980 = -0.2982 + 0.5 * 43.3908 + 0.01 * 0.0811\n",
      "-----------------\n",
      "Finished episode: 612 Reward: -1559.8568 total_loss = 22.0840 = -0.2959 + 0.5 * 44.7597 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 613 Reward: -1583.6155 total_loss = 19.0862 = -0.2815 + 0.5 * 38.7329 + 0.01 * 0.1198\n",
      "-----------------\n",
      "Finished episode: 614 Reward: -1565.2452 total_loss = 20.2362 = -0.3026 + 0.5 * 41.0776 + 0.01 * 0.0010\n",
      "-----------------\n",
      "Finished episode: 615 Reward: -1608.3537 total_loss = 19.8034 = -0.2989 + 0.5 * 40.2046 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 616 Reward: -1596.6933 total_loss = 18.9798 = -0.2827 + 0.5 * 38.5250 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 617 Reward: -1580.1861 total_loss = 19.7695 = -0.3237 + 0.5 * 40.1862 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 618 Reward: -1595.6359 total_loss = 19.2279 = -0.3035 + 0.5 * 39.0629 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 619 Reward: -1569.6892 total_loss = 17.7151 = -0.3040 + 0.5 * 36.0362 + 0.01 * 0.0930\n",
      "-----------------\n",
      "Finished episode: 620 Reward: -1593.2009 total_loss = 20.7048 = -0.3376 + 0.5 * 42.0850 + 0.01 * -0.0003\n",
      "-----------------\n",
      "Finished episode: 621 Reward: -1595.3782 total_loss = 18.7641 = -0.3234 + 0.5 * 38.1746 + 0.01 * 0.0175\n",
      "-----------------\n",
      "Finished episode: 622 Reward: -1582.9629 total_loss = 19.5741 = -0.3095 + 0.5 * 39.7672 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 623 Reward: -1605.2337 total_loss = 20.3619 = -0.3433 + 0.5 * 41.4104 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 624 Reward: -1576.3737 total_loss = 18.6315 = -0.3332 + 0.5 * 37.9295 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 625 Reward: -1598.0488 total_loss = 20.4346 = -0.3068 + 0.5 * 41.4823 + 0.01 * 0.0283\n",
      "-----------------\n",
      "Finished episode: 626 Reward: -1591.5251 total_loss = 20.5822 = -0.3134 + 0.5 * 41.7522 + 0.01 * 1.9557\n",
      "-----------------\n",
      "Finished episode: 627 Reward: -1583.2588 total_loss = 20.7094 = -0.3457 + 0.5 * 42.1080 + 0.01 * 0.1120\n",
      "-----------------\n",
      "Finished episode: 628 Reward: -1602.9340 total_loss = 19.9083 = -0.3255 + 0.5 * 40.4677 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 629 Reward: -1579.7968 total_loss = 18.5284 = -0.2914 + 0.5 * 37.6343 + 0.01 * 0.2587\n",
      "-----------------\n",
      "Finished episode: 630 Reward: -1598.7974 total_loss = 19.8089 = -0.3103 + 0.5 * 40.2384 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 631 Reward: -1601.3877 total_loss = 18.4016 = -0.3391 + 0.5 * 37.4787 + 0.01 * 0.1430\n",
      "-----------------\n",
      "Finished episode: 632 Reward: -1591.8510 total_loss = 21.2694 = -0.3374 + 0.5 * 43.2117 + 0.01 * 0.0921\n",
      "-----------------\n",
      "Finished episode: 633 Reward: -1600.2109 total_loss = 21.0640 = -0.3137 + 0.5 * 42.7521 + 0.01 * 0.1621\n",
      "-----------------\n",
      "Finished episode: 634 Reward: -1581.4726 total_loss = 19.7195 = -0.3693 + 0.5 * 40.1766 + 0.01 * 0.0444\n",
      "-----------------\n",
      "Finished episode: 635 Reward: -1599.5218 total_loss = 20.5765 = -0.3388 + 0.5 * 41.8305 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 636 Reward: -1594.8922 total_loss = 18.9292 = -0.3183 + 0.5 * 38.4950 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 637 Reward: -1598.8708 total_loss = 18.6819 = -0.3156 + 0.5 * 37.9950 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 638 Reward: -1597.3874 total_loss = 20.9467 = -0.3155 + 0.5 * 42.5244 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 639 Reward: -1583.2682 total_loss = 19.1461 = -0.3004 + 0.5 * 38.8907 + 0.01 * 0.1132\n",
      "-----------------\n",
      "Finished episode: 640 Reward: -1582.0844 total_loss = 21.0725 = -0.3049 + 0.5 * 42.7549 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 641 Reward: -1610.8640 total_loss = 22.2740 = -0.3635 + 0.5 * 45.2751 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 642 Reward: -1590.8391 total_loss = 20.9686 = -0.3017 + 0.5 * 42.5389 + 0.01 * 0.0860\n",
      "-----------------\n",
      "Finished episode: 643 Reward: -1594.7416 total_loss = 20.3815 = -0.3103 + 0.5 * 41.3830 + 0.01 * 0.0272\n",
      "-----------------\n",
      "Finished episode: 644 Reward: -1584.1166 total_loss = 19.2031 = -0.3301 + 0.5 * 39.0665 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 645 Reward: -1579.8658 total_loss = 20.5043 = -0.3022 + 0.5 * 41.6105 + 0.01 * 0.1265\n",
      "-----------------\n",
      "Finished episode: 646 Reward: -1560.1394 total_loss = 19.1809 = -0.2942 + 0.5 * 38.9503 + 0.01 * -0.0040\n",
      "-----------------\n",
      "Finished episode: 647 Reward: -1578.1584 total_loss = 22.2165 = -0.3356 + 0.5 * 45.1042 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 648 Reward: -1606.8114 total_loss = 20.3653 = -0.2976 + 0.5 * 41.3257 + 0.01 * -0.0008\n",
      "-----------------\n",
      "Finished episode: 649 Reward: -1584.7109 total_loss = 19.6615 = -0.3093 + 0.5 * 39.9416 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 650 Reward: -1607.1907 total_loss = 21.5212 = -0.3509 + 0.5 * 43.7442 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 651 Reward: -1573.2142 total_loss = 18.9188 = -0.2908 + 0.5 * 38.4061 + 0.01 * 0.6507\n",
      "-----------------\n",
      "Finished episode: 652 Reward: -1602.6253 total_loss = 19.5409 = -0.3255 + 0.5 * 39.7328 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 653 Reward: -1568.9481 total_loss = 19.6582 = -0.3503 + 0.5 * 40.0170 + 0.01 * -0.0009\n",
      "-----------------\n",
      "Finished episode: 654 Reward: -1595.1156 total_loss = 21.0122 = -0.3339 + 0.5 * 42.6658 + 0.01 * 1.3096\n",
      "-----------------\n",
      "Finished episode: 655 Reward: -1594.7753 total_loss = 20.1886 = -0.3058 + 0.5 * 40.9867 + 0.01 * 0.1033\n",
      "-----------------\n",
      "Finished episode: 656 Reward: -1574.6545 total_loss = 18.6764 = -0.3334 + 0.5 * 38.0183 + 0.01 * 0.0671\n",
      "-----------------\n",
      "Finished episode: 657 Reward: -1580.7586 total_loss = 21.1695 = -0.2921 + 0.5 * 42.9172 + 0.01 * 0.2940\n",
      "-----------------\n",
      "Finished episode: 658 Reward: -1581.8226 total_loss = 18.5410 = -0.2819 + 0.5 * 37.6457 + 0.01 * -0.0005\n",
      "-----------------\n",
      "Finished episode: 659 Reward: -1604.0114 total_loss = 19.4470 = -0.3066 + 0.5 * 39.5047 + 0.01 * 0.1244\n",
      "-----------------\n",
      "Finished episode: 660 Reward: -1588.8951 total_loss = 20.3691 = -0.3144 + 0.5 * 41.3668 + 0.01 * 0.0029\n",
      "-----------------\n",
      "Finished episode: 661 Reward: -1583.1580 total_loss = 19.6548 = -0.2906 + 0.5 * 39.8909 + 0.01 * -0.0026\n",
      "-----------------\n",
      "Finished episode: 662 Reward: -1599.5035 total_loss = 18.4324 = -0.3022 + 0.5 * 37.4623 + 0.01 * 0.3435\n",
      "-----------------\n",
      "Finished episode: 663 Reward: -1604.0775 total_loss = 21.3657 = -0.3033 + 0.5 * 43.3380 + 0.01 * -0.0005\n",
      "-----------------\n",
      "Finished episode: 664 Reward: -1578.8490 total_loss = 20.9677 = -0.3363 + 0.5 * 42.6081 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 665 Reward: -1585.8839 total_loss = 19.8686 = -0.3208 + 0.5 * 40.3788 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 666 Reward: -1589.2395 total_loss = 19.1665 = -0.3171 + 0.5 * 38.9672 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 667 Reward: -1583.0211 total_loss = 18.1816 = -0.5355 + 0.5 * 37.4312 + 0.01 * 0.1447\n",
      "-----------------\n",
      "Finished episode: 668 Reward: -1580.0818 total_loss = 20.0369 = -0.3660 + 0.5 * 40.8035 + 0.01 * 0.1113\n",
      "-----------------\n",
      "Finished episode: 669 Reward: -1606.5500 total_loss = 21.3749 = -0.3028 + 0.5 * 43.3553 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 670 Reward: -1606.3941 total_loss = 19.5655 = -0.3388 + 0.5 * 39.8087 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 671 Reward: -1591.1237 total_loss = 20.6344 = -0.3242 + 0.5 * 41.9163 + 0.01 * 0.0442\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 672 Reward: -1590.0499 total_loss = 20.0027 = -0.3395 + 0.5 * 40.6835 + 0.01 * 0.0436\n",
      "-----------------\n",
      "Finished episode: 673 Reward: -1571.6949 total_loss = 20.6509 = -0.3159 + 0.5 * 41.9299 + 0.01 * 0.1820\n",
      "-----------------\n",
      "Finished episode: 674 Reward: -1602.7331 total_loss = 19.8916 = -0.3522 + 0.5 * 40.4841 + 0.01 * 0.1731\n",
      "-----------------\n",
      "Finished episode: 675 Reward: -1571.1324 total_loss = 21.7088 = -0.3238 + 0.5 * 44.0607 + 0.01 * 0.2172\n",
      "-----------------\n",
      "Finished episode: 676 Reward: -1583.7259 total_loss = 18.9006 = -0.2998 + 0.5 * 38.4007 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 677 Reward: -1593.5836 total_loss = 19.9486 = -0.3140 + 0.5 * 40.5252 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 678 Reward: -1569.0714 total_loss = 18.9283 = -0.3100 + 0.5 * 38.4765 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 679 Reward: -1561.4632 total_loss = 19.8589 = -0.3390 + 0.5 * 40.3948 + 0.01 * 0.0516\n",
      "-----------------\n",
      "Finished episode: 680 Reward: -1579.5002 total_loss = 20.3351 = -0.3161 + 0.5 * 41.3025 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 681 Reward: -1571.6455 total_loss = 17.4309 = -0.2958 + 0.5 * 35.4500 + 0.01 * 0.1699\n",
      "-----------------\n",
      "Finished episode: 682 Reward: -1588.4582 total_loss = 20.3275 = -0.2674 + 0.5 * 41.1898 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 683 Reward: -1575.8060 total_loss = 18.8843 = -0.3258 + 0.5 * 38.4203 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 684 Reward: -1584.4177 total_loss = 21.8117 = -0.2890 + 0.5 * 44.2014 + 0.01 * 0.0000\n",
      "-----------------\n",
      "Finished episode: 685 Reward: -1597.3979 total_loss = 20.5401 = -0.3256 + 0.5 * 41.7300 + 0.01 * 0.0715\n",
      "-----------------\n",
      "Finished episode: 686 Reward: -1576.3636 total_loss = 18.8729 = -0.3213 + 0.5 * 38.3883 + 0.01 * -0.0005\n",
      "-----------------\n",
      "Finished episode: 687 Reward: -1599.9575 total_loss = 20.5572 = -0.2976 + 0.5 * 41.7064 + 0.01 * 0.1539\n",
      "-----------------\n",
      "Finished episode: 688 Reward: -1587.3618 total_loss = 20.8466 = -0.2679 + 0.5 * 42.2291 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 689 Reward: -1555.9669 total_loss = 21.8424 = -0.3041 + 0.5 * 44.2931 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 690 Reward: -1592.6241 total_loss = 19.9468 = -0.3313 + 0.5 * 40.5562 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 691 Reward: -1562.8998 total_loss = 17.3281 = -0.2783 + 0.5 * 35.2129 + 0.01 * -0.0007\n",
      "-----------------\n",
      "Finished episode: 692 Reward: -1577.7646 total_loss = 20.2343 = -0.3023 + 0.5 * 41.0730 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 693 Reward: -1557.1648 total_loss = 19.1704 = -0.3097 + 0.5 * 38.9596 + 0.01 * 0.0337\n",
      "-----------------\n",
      "Finished episode: 694 Reward: -1574.3588 total_loss = 18.4488 = -0.3139 + 0.5 * 37.5143 + 0.01 * 0.5628\n",
      "-----------------\n",
      "Finished episode: 695 Reward: -1567.2672 total_loss = 18.3507 = -0.3306 + 0.5 * 37.3569 + 0.01 * 0.2811\n",
      "-----------------\n",
      "Finished episode: 696 Reward: -1557.3038 total_loss = 18.8031 = -0.3407 + 0.5 * 38.2755 + 0.01 * 0.6007\n",
      "-----------------\n",
      "Finished episode: 697 Reward: -1555.3208 total_loss = 18.2361 = -0.3376 + 0.5 * 37.1284 + 0.01 * 0.9509\n",
      "-----------------\n",
      "Finished episode: 698 Reward: -1549.3296 total_loss = 16.6633 = -1.5599 + 0.5 * 36.4397 + 0.01 * 0.3335\n",
      "-----------------\n",
      "Finished episode: 699 Reward: -1565.0394 total_loss = 17.9800 = -0.4621 + 0.5 * 36.8648 + 0.01 * 0.9711\n",
      "-----------------\n",
      "Finished episode: 700 Reward: -1543.3921 total_loss = 20.6782 = -0.4444 + 0.5 * 42.2241 + 0.01 * 1.0532\n",
      "-----------------\n",
      "Finished episode: 701 Reward: -1573.4331 total_loss = 19.9474 = -0.2662 + 0.5 * 40.4188 + 0.01 * 0.4180\n",
      "-----------------\n",
      "Finished episode: 702 Reward: -1575.5639 total_loss = 19.3054 = -0.3165 + 0.5 * 39.2320 + 0.01 * 0.5911\n",
      "-----------------\n",
      "Finished episode: 703 Reward: -1545.1561 total_loss = 20.6782 = -0.3447 + 0.5 * 42.0364 + 0.01 * 0.4741\n",
      "-----------------\n",
      "Finished episode: 704 Reward: -1582.9456 total_loss = 21.3340 = -0.3214 + 0.5 * 43.3021 + 0.01 * 0.4263\n",
      "-----------------\n",
      "Finished episode: 705 Reward: -1531.9379 total_loss = 17.9946 = -0.8380 + 0.5 * 37.6405 + 0.01 * 1.2318\n",
      "-----------------\n",
      "Finished episode: 706 Reward: -1566.9099 total_loss = 19.5170 = -0.3452 + 0.5 * 39.7166 + 0.01 * 0.3909\n",
      "-----------------\n",
      "Finished episode: 707 Reward: -1582.7035 total_loss = 20.6093 = -0.3072 + 0.5 * 41.8256 + 0.01 * 0.3701\n",
      "-----------------\n",
      "Finished episode: 708 Reward: -1570.1103 total_loss = 21.2196 = -0.3425 + 0.5 * 43.1090 + 0.01 * 0.7547\n",
      "-----------------\n",
      "Finished episode: 709 Reward: -1563.8499 total_loss = 19.7765 = -0.3254 + 0.5 * 40.1918 + 0.01 * 0.5990\n",
      "-----------------\n",
      "Finished episode: 710 Reward: -1569.9839 total_loss = 18.2664 = -0.2962 + 0.5 * 37.1251 + 0.01 * 0.0023\n",
      "-----------------\n",
      "Finished episode: 711 Reward: -1577.6619 total_loss = 19.3703 = -0.3248 + 0.5 * 39.3809 + 0.01 * 0.4590\n",
      "-----------------\n",
      "Finished episode: 712 Reward: -1576.7399 total_loss = 18.8555 = -0.2903 + 0.5 * 38.2838 + 0.01 * 0.3857\n",
      "-----------------\n",
      "Finished episode: 713 Reward: -1562.7315 total_loss = 19.5522 = -0.3265 + 0.5 * 39.7466 + 0.01 * 0.5443\n",
      "-----------------\n",
      "Finished episode: 714 Reward: -1556.7762 total_loss = 19.6212 = -0.2923 + 0.5 * 39.8182 + 0.01 * 0.4459\n",
      "-----------------\n",
      "Finished episode: 715 Reward: -1560.8646 total_loss = 20.7316 = -0.3239 + 0.5 * 42.1110 + 0.01 * 0.0007\n",
      "-----------------\n",
      "Finished episode: 716 Reward: -1522.5551 total_loss = 20.7988 = -0.2727 + 0.5 * 42.1342 + 0.01 * 0.4359\n",
      "-----------------\n",
      "Finished episode: 717 Reward: -1555.1036 total_loss = 21.6952 = -0.4252 + 0.5 * 44.2313 + 0.01 * 0.4730\n",
      "-----------------\n",
      "Finished episode: 718 Reward: -1534.9987 total_loss = 18.4675 = -0.3160 + 0.5 * 37.5668 + 0.01 * 0.0083\n",
      "-----------------\n",
      "Finished episode: 719 Reward: -1574.6429 total_loss = 18.8918 = -0.3243 + 0.5 * 38.4322 + 0.01 * -0.0011\n",
      "-----------------\n",
      "Finished episode: 720 Reward: -1586.2805 total_loss = 19.6429 = -0.3086 + 0.5 * 39.9029 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 721 Reward: -1564.5319 total_loss = 19.2227 = -0.3144 + 0.5 * 39.0743 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 722 Reward: -1545.2834 total_loss = 18.0145 = -0.3091 + 0.5 * 36.6472 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 723 Reward: -1547.9908 total_loss = 17.3279 = -0.3110 + 0.5 * 35.2778 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 724 Reward: -1556.0551 total_loss = 19.0093 = -0.3138 + 0.5 * 38.6462 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 725 Reward: -1529.4778 total_loss = 18.2650 = -0.3076 + 0.5 * 37.1449 + 0.01 * 0.0123\n",
      "-----------------\n",
      "Finished episode: 726 Reward: -1546.0469 total_loss = 21.9657 = -0.3239 + 0.5 * 44.5792 + 0.01 * -0.0003\n",
      "-----------------\n",
      "Finished episode: 727 Reward: -1567.1234 total_loss = 20.4973 = -0.2929 + 0.5 * 41.5804 + 0.01 * -0.0013\n",
      "-----------------\n",
      "Finished episode: 728 Reward: -1543.3969 total_loss = 17.9971 = -0.2941 + 0.5 * 36.5825 + 0.01 * -0.0044\n",
      "-----------------\n",
      "Finished episode: 729 Reward: -1558.3897 total_loss = 20.6933 = -0.3290 + 0.5 * 42.0448 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 730 Reward: -1559.6757 total_loss = 20.5082 = -0.3285 + 0.5 * 41.6704 + 0.01 * 0.1489\n",
      "-----------------\n",
      "Finished episode: 731 Reward: -1567.7777 total_loss = 19.5489 = -0.3013 + 0.5 * 39.7004 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 732 Reward: -1543.3625 total_loss = 19.5718 = -0.3336 + 0.5 * 39.8109 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 733 Reward: -1562.2490 total_loss = 20.3981 = -0.3150 + 0.5 * 41.4263 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 734 Reward: -1585.5404 total_loss = 18.8279 = -0.3316 + 0.5 * 38.3191 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 735 Reward: -1578.3574 total_loss = 20.2788 = -0.3293 + 0.5 * 41.2161 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 736 Reward: -1572.6026 total_loss = 18.9667 = -0.2987 + 0.5 * 38.5308 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 737 Reward: -1545.0701 total_loss = 18.7806 = -0.3178 + 0.5 * 38.1968 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 738 Reward: -1520.2993 total_loss = 18.6279 = -0.2913 + 0.5 * 37.8303 + 0.01 * 0.4010\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 739 Reward: -1532.7169 total_loss = 18.3652 = -0.3086 + 0.5 * 37.3478 + 0.01 * -0.0007\n",
      "-----------------\n",
      "Finished episode: 740 Reward: -1454.6163 total_loss = 17.1703 = -0.4418 + 0.5 * 35.2006 + 0.01 * 1.1754\n",
      "-----------------\n",
      "Finished episode: 741 Reward: -1544.3691 total_loss = 18.5033 = -0.3144 + 0.5 * 37.6354 + 0.01 * -0.0015\n",
      "-----------------\n",
      "Finished episode: 742 Reward: -1551.8145 total_loss = 18.6912 = -0.3372 + 0.5 * 38.0567 + 0.01 * -0.0015\n",
      "-----------------\n",
      "Finished episode: 743 Reward: -1566.8441 total_loss = 19.4206 = -0.2933 + 0.5 * 39.4277 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 744 Reward: -1567.3505 total_loss = 19.4259 = -0.2931 + 0.5 * 39.4380 + 0.01 * -0.0012\n",
      "-----------------\n",
      "Finished episode: 745 Reward: -1568.0573 total_loss = 19.3789 = -0.3141 + 0.5 * 39.3860 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 746 Reward: -1568.0968 total_loss = 20.2468 = -0.3182 + 0.5 * 41.1300 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 747 Reward: -1573.1739 total_loss = 17.6857 = -0.3358 + 0.5 * 36.0378 + 0.01 * 0.2555\n",
      "-----------------\n",
      "Finished episode: 748 Reward: -1545.9835 total_loss = 17.7149 = -0.3229 + 0.5 * 36.0695 + 0.01 * 0.3109\n",
      "-----------------\n",
      "Finished episode: 749 Reward: -1553.6098 total_loss = 18.4136 = -0.3341 + 0.5 * 37.4954 + 0.01 * -0.0003\n",
      "-----------------\n",
      "Finished episode: 750 Reward: -1559.6051 total_loss = 19.8879 = -0.3001 + 0.5 * 40.3759 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 751 Reward: -1542.8708 total_loss = 19.3211 = -0.3326 + 0.5 * 39.3075 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 752 Reward: -1578.8152 total_loss = 18.7747 = -0.3103 + 0.5 * 38.1700 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 753 Reward: -1561.7591 total_loss = 20.1123 = -0.3307 + 0.5 * 40.8861 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 754 Reward: -1574.6528 total_loss = 20.5460 = -0.3148 + 0.5 * 41.7215 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 755 Reward: -1559.6603 total_loss = 19.7153 = -0.3183 + 0.5 * 40.0671 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 756 Reward: -1557.4480 total_loss = 17.9977 = -0.3038 + 0.5 * 36.6028 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 757 Reward: -1553.3658 total_loss = 18.6225 = -0.2996 + 0.5 * 37.8443 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 758 Reward: -1582.2368 total_loss = 19.8131 = -0.3188 + 0.5 * 40.2638 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 759 Reward: -1546.7013 total_loss = 19.4259 = -0.3332 + 0.5 * 39.5182 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 760 Reward: -1577.6230 total_loss = 20.8982 = -0.3058 + 0.5 * 42.4075 + 0.01 * 0.0247\n",
      "-----------------\n",
      "Finished episode: 761 Reward: -1513.9631 total_loss = 18.2743 = -0.3004 + 0.5 * 37.1493 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 762 Reward: -1540.4329 total_loss = 18.3256 = -0.3248 + 0.5 * 37.3008 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 763 Reward: -1566.6319 total_loss = 19.4936 = -0.2765 + 0.5 * 39.5401 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 764 Reward: -1568.6086 total_loss = 20.0261 = -0.3038 + 0.5 * 40.6598 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 765 Reward: -1530.9196 total_loss = 21.4124 = -0.3130 + 0.5 * 43.4375 + 0.01 * 0.6700\n",
      "-----------------\n",
      "Finished episode: 766 Reward: -1549.3825 total_loss = 19.6846 = -0.3132 + 0.5 * 39.9897 + 0.01 * 0.2926\n",
      "-----------------\n",
      "Finished episode: 767 Reward: -1541.6329 total_loss = 18.2010 = -0.3061 + 0.5 * 37.0141 + 0.01 * 0.0069\n",
      "-----------------\n",
      "Finished episode: 768 Reward: -1540.3561 total_loss = 20.0728 = -0.3366 + 0.5 * 40.8172 + 0.01 * 0.0812\n",
      "-----------------\n",
      "Finished episode: 769 Reward: -1574.1933 total_loss = 18.6217 = -0.3291 + 0.5 * 37.9016 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 770 Reward: -1544.2552 total_loss = 19.1803 = -0.2813 + 0.5 * 38.9232 + 0.01 * -0.0032\n",
      "-----------------\n",
      "Finished episode: 771 Reward: -1544.8562 total_loss = 17.6106 = -0.2971 + 0.5 * 35.8153 + 0.01 * -0.0005\n",
      "-----------------\n",
      "Finished episode: 772 Reward: -1576.8022 total_loss = 20.7981 = -0.3383 + 0.5 * 42.2727 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 773 Reward: -1573.4986 total_loss = 19.7762 = -0.3367 + 0.5 * 40.2192 + 0.01 * 0.3361\n",
      "-----------------\n",
      "Finished episode: 774 Reward: -1565.1976 total_loss = 18.4045 = -0.3293 + 0.5 * 37.4673 + 0.01 * 0.0213\n",
      "-----------------\n",
      "Finished episode: 775 Reward: -1520.4911 total_loss = 19.2598 = -0.2891 + 0.5 * 39.0922 + 0.01 * 0.2874\n",
      "-----------------\n",
      "Finished episode: 776 Reward: -1548.9356 total_loss = 19.1055 = -0.3541 + 0.5 * 38.9193 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 777 Reward: -1540.1489 total_loss = 17.8010 = -0.3213 + 0.5 * 36.2447 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 778 Reward: -1566.9736 total_loss = 19.8790 = -0.3286 + 0.5 * 40.4153 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 779 Reward: -1577.9250 total_loss = 19.3276 = -0.3394 + 0.5 * 39.3341 + 0.01 * -0.0010\n",
      "-----------------\n",
      "Finished episode: 780 Reward: -1546.8473 total_loss = 18.2996 = -0.3258 + 0.5 * 37.2508 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 781 Reward: -1576.2008 total_loss = 18.4228 = -0.3205 + 0.5 * 37.4867 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 782 Reward: -1552.3286 total_loss = 19.5309 = -0.3332 + 0.5 * 39.7283 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 783 Reward: -1573.2261 total_loss = 19.8415 = -0.3069 + 0.5 * 40.2969 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 784 Reward: -1564.7416 total_loss = 19.9037 = -0.3091 + 0.5 * 40.4256 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 785 Reward: -1559.1177 total_loss = 21.1019 = -0.3231 + 0.5 * 42.8447 + 0.01 * 0.2641\n",
      "-----------------\n",
      "Finished episode: 786 Reward: -1577.2352 total_loss = 19.5940 = -0.3378 + 0.5 * 39.8608 + 0.01 * 0.1325\n",
      "-----------------\n",
      "Finished episode: 787 Reward: -1537.3887 total_loss = 17.8440 = -0.3278 + 0.5 * 36.3430 + 0.01 * 0.0229\n",
      "-----------------\n",
      "Finished episode: 788 Reward: -1590.9667 total_loss = 20.5877 = -0.3673 + 0.5 * 41.9100 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 789 Reward: -1573.3001 total_loss = 19.5477 = -0.3143 + 0.5 * 39.7224 + 0.01 * 0.0901\n",
      "-----------------\n",
      "Finished episode: 790 Reward: -1577.9781 total_loss = 21.3220 = -0.3011 + 0.5 * 43.2449 + 0.01 * 0.0566\n",
      "-----------------\n",
      "Finished episode: 791 Reward: -1588.3432 total_loss = 18.5910 = -0.2805 + 0.5 * 37.7429 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 792 Reward: -1593.9348 total_loss = 17.2179 = -0.2825 + 0.5 * 34.9997 + 0.01 * 0.0603\n",
      "-----------------\n",
      "Finished episode: 793 Reward: -1562.5231 total_loss = 18.1959 = -0.2973 + 0.5 * 36.9865 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 794 Reward: -1593.2815 total_loss = 19.4442 = -0.3116 + 0.5 * 39.5116 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 795 Reward: -1590.8529 total_loss = 19.4977 = -0.3290 + 0.5 * 39.6468 + 0.01 * 0.3378\n",
      "-----------------\n",
      "Finished episode: 796 Reward: -1586.4378 total_loss = 21.3761 = -0.3096 + 0.5 * 43.3715 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 797 Reward: -1575.1099 total_loss = 19.8909 = -0.3041 + 0.5 * 40.3879 + 0.01 * 0.0985\n",
      "-----------------\n",
      "Finished episode: 798 Reward: -1591.2429 total_loss = 19.6242 = -0.3163 + 0.5 * 39.8774 + 0.01 * 0.1825\n",
      "-----------------\n",
      "Finished episode: 799 Reward: -1564.2793 total_loss = 20.0100 = -0.2963 + 0.5 * 40.6128 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 800 Reward: -1569.4413 total_loss = 19.4807 = -0.3570 + 0.5 * 39.6693 + 0.01 * 0.3029\n",
      "-----------------\n",
      "Finished episode: 801 Reward: -1582.9313 total_loss = 19.7920 = -0.2982 + 0.5 * 40.1805 + 0.01 * -0.0004\n",
      "-----------------\n",
      "Finished episode: 802 Reward: -1584.0959 total_loss = 20.7229 = -0.3175 + 0.5 * 42.0809 + 0.01 * -0.0003\n",
      "-----------------\n",
      "Finished episode: 803 Reward: -1580.0063 total_loss = 19.1467 = -0.3001 + 0.5 * 38.8936 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 804 Reward: -1569.2681 total_loss = 18.9759 = -0.3020 + 0.5 * 38.5557 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 805 Reward: -1585.2877 total_loss = 18.9963 = -0.3530 + 0.5 * 38.6877 + 0.01 * 0.5441\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 806 Reward: -1591.9633 total_loss = 20.9469 = -0.3125 + 0.5 * 42.5083 + 0.01 * 0.5305\n",
      "-----------------\n",
      "Finished episode: 807 Reward: -1582.4556 total_loss = 18.9359 = -0.3910 + 0.5 * 38.6362 + 0.01 * 0.8779\n",
      "-----------------\n",
      "Finished episode: 808 Reward: -1565.7728 total_loss = 18.9029 = -0.3581 + 0.5 * 38.5220 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 809 Reward: -1550.1313 total_loss = 19.4785 = -0.3426 + 0.5 * 39.6156 + 0.01 * 1.3287\n",
      "-----------------\n",
      "Finished episode: 810 Reward: -1570.4997 total_loss = 18.6101 = -0.2955 + 0.5 * 37.7899 + 0.01 * 1.0628\n",
      "-----------------\n",
      "Finished episode: 811 Reward: -1568.4874 total_loss = 18.2227 = -0.2775 + 0.5 * 36.9453 + 0.01 * 2.7472\n",
      "-----------------\n",
      "Finished episode: 812 Reward: -1561.3845 total_loss = 19.4028 = -0.3080 + 0.5 * 39.4125 + 0.01 * 0.4582\n",
      "-----------------\n",
      "Finished episode: 813 Reward: -1561.6114 total_loss = 16.9683 = -0.4392 + 0.5 * 34.7860 + 0.01 * 1.4440\n",
      "-----------------\n",
      "Finished episode: 814 Reward: -1549.9608 total_loss = 19.8334 = -0.3197 + 0.5 * 40.2932 + 0.01 * 0.6562\n",
      "-----------------\n",
      "Finished episode: 815 Reward: -1569.0502 total_loss = 17.6390 = -0.3181 + 0.5 * 35.9096 + 0.01 * 0.2387\n",
      "-----------------\n",
      "Finished episode: 816 Reward: -1533.2797 total_loss = 18.7707 = -0.3031 + 0.5 * 38.1212 + 0.01 * 1.3254\n",
      "-----------------\n",
      "Finished episode: 817 Reward: -1552.3026 total_loss = 19.5106 = -0.3115 + 0.5 * 39.6441 + 0.01 * -0.0030\n",
      "-----------------\n",
      "Finished episode: 818 Reward: -1568.7056 total_loss = 19.2652 = -0.3499 + 0.5 * 39.2235 + 0.01 * 0.3365\n",
      "-----------------\n",
      "Finished episode: 819 Reward: -1546.3824 total_loss = 20.3824 = -0.3169 + 0.5 * 41.3986 + 0.01 * -0.0063\n",
      "-----------------\n",
      "Finished episode: 820 Reward: -1523.7901 total_loss = 17.8495 = -0.3534 + 0.5 * 36.3878 + 0.01 * 0.9118\n",
      "-----------------\n",
      "Finished episode: 821 Reward: -1553.8661 total_loss = 18.8746 = -0.4296 + 0.5 * 38.6007 + 0.01 * 0.3857\n",
      "-----------------\n",
      "Finished episode: 822 Reward: -1551.5419 total_loss = 19.2774 = -0.3190 + 0.5 * 39.1866 + 0.01 * 0.3107\n",
      "-----------------\n",
      "Finished episode: 823 Reward: -1569.9951 total_loss = 19.9546 = -0.3009 + 0.5 * 40.4820 + 0.01 * 1.4473\n",
      "-----------------\n",
      "Finished episode: 824 Reward: -1561.4961 total_loss = 18.4137 = -0.3128 + 0.5 * 37.4471 + 0.01 * 0.2895\n",
      "-----------------\n",
      "Finished episode: 825 Reward: -1561.3147 total_loss = 18.6722 = -0.4161 + 0.5 * 38.1646 + 0.01 * 0.5990\n",
      "-----------------\n",
      "Finished episode: 826 Reward: -1555.5798 total_loss = 17.3299 = -0.3214 + 0.5 * 35.2945 + 0.01 * 0.4033\n",
      "-----------------\n",
      "Finished episode: 827 Reward: -1549.5493 total_loss = 19.9275 = -0.3447 + 0.5 * 40.5278 + 0.01 * 0.8252\n",
      "-----------------\n",
      "Finished episode: 828 Reward: -1545.7875 total_loss = 17.5105 = -0.3305 + 0.5 * 35.6546 + 0.01 * 1.3673\n",
      "-----------------\n",
      "Finished episode: 829 Reward: -1554.3036 total_loss = 19.2099 = -0.3913 + 0.5 * 39.1803 + 0.01 * 1.1033\n",
      "-----------------\n",
      "Finished episode: 830 Reward: -1556.6537 total_loss = 18.5807 = -0.3201 + 0.5 * 37.7991 + 0.01 * 0.1266\n",
      "-----------------\n",
      "Finished episode: 831 Reward: -1525.2190 total_loss = 19.8736 = -0.3338 + 0.5 * 40.3978 + 0.01 * 0.8497\n",
      "-----------------\n",
      "Finished episode: 832 Reward: -1551.1987 total_loss = 19.8077 = -0.3428 + 0.5 * 40.2880 + 0.01 * 0.6454\n",
      "-----------------\n",
      "Finished episode: 833 Reward: -1553.7531 total_loss = 18.7446 = -0.2485 + 0.5 * 37.9801 + 0.01 * 0.2997\n",
      "-----------------\n",
      "Finished episode: 834 Reward: -1562.5663 total_loss = 17.9947 = -0.3172 + 0.5 * 36.6213 + 0.01 * 0.1263\n",
      "-----------------\n",
      "Finished episode: 835 Reward: -1510.5049 total_loss = 19.0607 = -0.3311 + 0.5 * 38.7740 + 0.01 * 0.4753\n",
      "-----------------\n",
      "Finished episode: 836 Reward: -1532.2681 total_loss = 19.4580 = -0.3956 + 0.5 * 39.6947 + 0.01 * 0.6263\n",
      "-----------------\n",
      "Finished episode: 837 Reward: -1552.2605 total_loss = 20.0829 = -0.2984 + 0.5 * 40.7581 + 0.01 * 0.2253\n",
      "-----------------\n",
      "Finished episode: 838 Reward: -1574.8823 total_loss = 18.5002 = -0.3858 + 0.5 * 37.7641 + 0.01 * 0.4021\n",
      "-----------------\n",
      "Finished episode: 839 Reward: -1566.1071 total_loss = 18.4901 = -0.3168 + 0.5 * 37.6022 + 0.01 * 0.5784\n",
      "-----------------\n",
      "Finished episode: 840 Reward: -1569.0788 total_loss = 19.9172 = -0.9759 + 0.5 * 41.7727 + 0.01 * 0.6760\n",
      "-----------------\n",
      "Finished episode: 841 Reward: -1491.7718 total_loss = 18.7608 = -0.2925 + 0.5 * 38.1031 + 0.01 * 0.1709\n",
      "-----------------\n",
      "Finished episode: 842 Reward: -1542.8408 total_loss = 17.6285 = -0.2996 + 0.5 * 35.8514 + 0.01 * 0.2377\n",
      "-----------------\n",
      "Finished episode: 843 Reward: -1524.4856 total_loss = 15.6201 = -0.3657 + 0.5 * 31.9471 + 0.01 * 1.2202\n",
      "-----------------\n",
      "Finished episode: 844 Reward: -1538.7737 total_loss = 18.1045 = -0.4582 + 0.5 * 37.1106 + 0.01 * 0.7431\n",
      "-----------------\n",
      "Finished episode: 845 Reward: -1507.9318 total_loss = 18.5695 = -0.3432 + 0.5 * 37.8214 + 0.01 * 0.2029\n",
      "-----------------\n",
      "Finished episode: 846 Reward: -1540.3200 total_loss = 18.1869 = -0.3138 + 0.5 * 36.9932 + 0.01 * 0.4087\n",
      "-----------------\n",
      "Finished episode: 847 Reward: -1533.0311 total_loss = 19.3115 = -0.3458 + 0.5 * 39.3093 + 0.01 * 0.2714\n",
      "-----------------\n",
      "Finished episode: 848 Reward: -1552.2908 total_loss = 18.5490 = -0.3292 + 0.5 * 37.7460 + 0.01 * 0.5134\n",
      "-----------------\n",
      "Finished episode: 849 Reward: -1556.3263 total_loss = 18.3965 = -0.3230 + 0.5 * 37.4334 + 0.01 * 0.2826\n",
      "-----------------\n",
      "Finished episode: 850 Reward: -1515.1597 total_loss = 17.6430 = -0.3110 + 0.5 * 35.9011 + 0.01 * 0.3448\n",
      "-----------------\n",
      "Finished episode: 851 Reward: -1554.6089 total_loss = 18.7323 = -0.3243 + 0.5 * 38.1126 + 0.01 * 0.0256\n",
      "-----------------\n",
      "Finished episode: 852 Reward: -1592.2266 total_loss = 20.4157 = -0.3302 + 0.5 * 41.4918 + 0.01 * -0.0037\n",
      "-----------------\n",
      "Finished episode: 853 Reward: -1576.5226 total_loss = 18.2549 = -0.3025 + 0.5 * 37.1141 + 0.01 * 0.0336\n",
      "-----------------\n",
      "Finished episode: 854 Reward: -1579.5348 total_loss = 17.5830 = -0.3293 + 0.5 * 35.8246 + 0.01 * -0.0014\n",
      "-----------------\n",
      "Finished episode: 855 Reward: -1543.8524 total_loss = 17.7222 = -0.3215 + 0.5 * 36.0820 + 0.01 * 0.2695\n",
      "-----------------\n",
      "Finished episode: 856 Reward: -1549.8970 total_loss = 18.7978 = -0.4155 + 0.5 * 38.4014 + 0.01 * 1.2615\n",
      "-----------------\n",
      "Finished episode: 857 Reward: -1559.4287 total_loss = 20.3746 = -0.3479 + 0.5 * 41.4302 + 0.01 * 0.7355\n",
      "-----------------\n",
      "Finished episode: 858 Reward: -1524.7723 total_loss = 16.2206 = -0.2835 + 0.5 * 33.0051 + 0.01 * 0.1597\n",
      "-----------------\n",
      "Finished episode: 859 Reward: -1558.8674 total_loss = 17.5142 = -0.2752 + 0.5 * 35.5757 + 0.01 * 0.1576\n",
      "-----------------\n",
      "Finished episode: 860 Reward: -1544.8678 total_loss = 18.0536 = -0.2965 + 0.5 * 36.6946 + 0.01 * 0.2806\n",
      "-----------------\n",
      "Finished episode: 861 Reward: -1578.4289 total_loss = 19.7275 = -0.3342 + 0.5 * 40.1165 + 0.01 * 0.3470\n",
      "-----------------\n",
      "Finished episode: 862 Reward: -1558.3731 total_loss = 16.5725 = -0.3438 + 0.5 * 33.8251 + 0.01 * 0.3752\n",
      "-----------------\n",
      "Finished episode: 863 Reward: -1556.9892 total_loss = 19.2470 = -0.3135 + 0.5 * 39.1188 + 0.01 * 0.1037\n",
      "-----------------\n",
      "Finished episode: 864 Reward: -1525.0068 total_loss = 16.8119 = -0.3261 + 0.5 * 34.2722 + 0.01 * 0.1968\n",
      "-----------------\n",
      "Finished episode: 865 Reward: -1541.3125 total_loss = 18.7757 = -0.3083 + 0.5 * 38.1664 + 0.01 * 0.0788\n",
      "-----------------\n",
      "Finished episode: 866 Reward: -1550.6146 total_loss = 18.6178 = -0.2990 + 0.5 * 37.8215 + 0.01 * 0.6038\n",
      "-----------------\n",
      "Finished episode: 867 Reward: -1528.7176 total_loss = 17.8115 = -0.3048 + 0.5 * 36.2263 + 0.01 * 0.3137\n",
      "-----------------\n",
      "Finished episode: 868 Reward: -1564.1038 total_loss = 19.3871 = -0.2963 + 0.5 * 39.3669 + 0.01 * -0.0088\n",
      "-----------------\n",
      "Finished episode: 869 Reward: -1552.2018 total_loss = 17.4871 = -0.3298 + 0.5 * 35.6307 + 0.01 * 0.1531\n",
      "-----------------\n",
      "Finished episode: 870 Reward: -1505.8824 total_loss = 17.2307 = -0.2635 + 0.5 * 34.9797 + 0.01 * 0.4394\n",
      "-----------------\n",
      "Finished episode: 871 Reward: -1555.9691 total_loss = 18.1407 = -0.3631 + 0.5 * 36.9923 + 0.01 * 0.7605\n",
      "-----------------\n",
      "Finished episode: 872 Reward: -1551.6924 total_loss = 16.4477 = -0.3290 + 0.5 * 33.5453 + 0.01 * 0.4051\n",
      "-----------------\n",
      "Finished episode: 873 Reward: -1544.3144 total_loss = 18.8849 = -0.3074 + 0.5 * 38.3755 + 0.01 * 0.4561\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 874 Reward: -1566.9732 total_loss = 20.4225 = -0.2683 + 0.5 * 41.3799 + 0.01 * 0.0812\n",
      "-----------------\n",
      "Finished episode: 875 Reward: -1553.5623 total_loss = 17.5541 = -0.3218 + 0.5 * 35.7451 + 0.01 * 0.3328\n",
      "-----------------\n",
      "Finished episode: 876 Reward: -1516.0604 total_loss = 16.4444 = -0.3523 + 0.5 * 33.5852 + 0.01 * 0.4161\n",
      "-----------------\n",
      "Finished episode: 877 Reward: -1570.4337 total_loss = 20.7457 = -0.3226 + 0.5 * 42.1311 + 0.01 * 0.2815\n",
      "-----------------\n",
      "Finished episode: 878 Reward: -1531.6499 total_loss = 18.1689 = -0.3223 + 0.5 * 36.9823 + 0.01 * 0.0104\n",
      "-----------------\n",
      "Finished episode: 879 Reward: -1505.1171 total_loss = 19.1973 = -0.3084 + 0.5 * 39.0002 + 0.01 * 0.5564\n",
      "-----------------\n",
      "Finished episode: 880 Reward: -1550.0879 total_loss = 19.6558 = -0.3278 + 0.5 * 39.9650 + 0.01 * 0.1183\n",
      "-----------------\n",
      "Finished episode: 881 Reward: -1537.9043 total_loss = 18.4472 = -0.3325 + 0.5 * 37.5560 + 0.01 * 0.1701\n",
      "-----------------\n",
      "Finished episode: 882 Reward: -1530.3032 total_loss = 20.1261 = -0.3257 + 0.5 * 40.8896 + 0.01 * 0.6953\n",
      "-----------------\n",
      "Finished episode: 883 Reward: -1540.2288 total_loss = 17.0279 = -0.2973 + 0.5 * 34.6483 + 0.01 * 0.0970\n",
      "-----------------\n",
      "Finished episode: 884 Reward: -1531.7070 total_loss = 16.4456 = -0.3049 + 0.5 * 33.4986 + 0.01 * 0.1238\n",
      "-----------------\n",
      "Finished episode: 885 Reward: -1555.1103 total_loss = 18.7983 = -0.2677 + 0.5 * 38.1312 + 0.01 * 0.0433\n",
      "-----------------\n",
      "Finished episode: 886 Reward: -1547.2477 total_loss = 19.4258 = -0.3091 + 0.5 * 39.4690 + 0.01 * 0.0371\n",
      "-----------------\n",
      "Finished episode: 887 Reward: -1568.8679 total_loss = 19.3220 = -0.3488 + 0.5 * 39.3283 + 0.01 * 0.6577\n",
      "-----------------\n",
      "Finished episode: 888 Reward: -1575.6328 total_loss = 19.5001 = -0.3810 + 0.5 * 39.7232 + 0.01 * 1.9517\n",
      "-----------------\n",
      "Finished episode: 889 Reward: -1558.0129 total_loss = 19.6378 = -0.3200 + 0.5 * 39.9155 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 890 Reward: -1558.4117 total_loss = 21.4537 = -0.2904 + 0.5 * 43.4882 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 891 Reward: -1573.1327 total_loss = 19.0905 = -0.5152 + 0.5 * 39.2050 + 0.01 * 0.3190\n",
      "-----------------\n",
      "Finished episode: 892 Reward: -1564.3552 total_loss = 20.6383 = -0.3154 + 0.5 * 41.9074 + 0.01 * -0.0023\n",
      "-----------------\n",
      "Finished episode: 893 Reward: -1529.0215 total_loss = 19.3897 = -0.3171 + 0.5 * 39.4030 + 0.01 * 0.5322\n",
      "-----------------\n",
      "Finished episode: 894 Reward: -1564.0081 total_loss = 20.1668 = -0.3546 + 0.5 * 41.0304 + 0.01 * 0.6242\n",
      "-----------------\n",
      "Finished episode: 895 Reward: -1543.3186 total_loss = 20.2006 = -0.3413 + 0.5 * 41.0779 + 0.01 * 0.2923\n",
      "-----------------\n",
      "Finished episode: 896 Reward: -1553.1054 total_loss = 21.1283 = -0.3023 + 0.5 * 42.8541 + 0.01 * 0.3524\n",
      "-----------------\n",
      "Finished episode: 897 Reward: -1536.2134 total_loss = 19.6071 = -0.3245 + 0.5 * 39.8631 + 0.01 * 0.0047\n",
      "-----------------\n",
      "Finished episode: 898 Reward: -1569.3316 total_loss = 19.2480 = -0.3448 + 0.5 * 39.1822 + 0.01 * 0.1653\n",
      "-----------------\n",
      "Finished episode: 899 Reward: -1546.3379 total_loss = 16.4911 = -0.3049 + 0.5 * 33.5909 + 0.01 * 0.0553\n",
      "-----------------\n",
      "Finished episode: 900 Reward: -1564.3259 total_loss = 20.0442 = -0.3274 + 0.5 * 40.7363 + 0.01 * 0.3436\n",
      "-----------------\n",
      "Finished episode: 901 Reward: -1555.1275 total_loss = 17.6923 = -0.3057 + 0.5 * 35.9933 + 0.01 * 0.1380\n",
      "-----------------\n",
      "Finished episode: 902 Reward: -1535.4385 total_loss = 18.8081 = -0.3203 + 0.5 * 38.2551 + 0.01 * 0.0867\n",
      "-----------------\n",
      "Finished episode: 903 Reward: -1552.4127 total_loss = 17.4511 = -0.3173 + 0.5 * 35.5322 + 0.01 * 0.2220\n",
      "-----------------\n",
      "Finished episode: 904 Reward: -1566.3207 total_loss = 17.8080 = -0.3438 + 0.5 * 36.2954 + 0.01 * 0.4042\n",
      "-----------------\n",
      "Finished episode: 905 Reward: -1538.3088 total_loss = 19.4348 = -0.3469 + 0.5 * 39.5475 + 0.01 * 0.7924\n",
      "-----------------\n",
      "Finished episode: 906 Reward: -1530.6179 total_loss = 19.6863 = -0.3571 + 0.5 * 40.0637 + 0.01 * 1.1582\n",
      "-----------------\n",
      "Finished episode: 907 Reward: -1514.4072 total_loss = 17.4118 = -0.2980 + 0.5 * 35.4195 + 0.01 * -0.0000\n",
      "-----------------\n",
      "Finished episode: 908 Reward: -1533.0262 total_loss = 20.3361 = -0.3786 + 0.5 * 41.4197 + 0.01 * 0.4900\n",
      "-----------------\n",
      "Finished episode: 909 Reward: -1569.3584 total_loss = 20.3447 = -0.3941 + 0.5 * 41.4628 + 0.01 * 0.7411\n",
      "-----------------\n",
      "Finished episode: 910 Reward: -1569.2071 total_loss = 18.6302 = -0.3849 + 0.5 * 37.9872 + 0.01 * 2.1491\n",
      "-----------------\n",
      "Finished episode: 911 Reward: -1556.5612 total_loss = 18.7275 = -0.3163 + 0.5 * 38.0658 + 0.01 * 1.0942\n",
      "-----------------\n",
      "Finished episode: 912 Reward: -1567.0991 total_loss = 20.9747 = -0.2857 + 0.5 * 42.5147 + 0.01 * 0.2978\n",
      "-----------------\n",
      "Finished episode: 913 Reward: -1549.7531 total_loss = 19.7202 = -0.3502 + 0.5 * 40.1387 + 0.01 * 0.1017\n",
      "-----------------\n",
      "Finished episode: 914 Reward: -1562.9047 total_loss = 18.4979 = -0.3312 + 0.5 * 37.6460 + 0.01 * 0.6060\n",
      "-----------------\n",
      "Finished episode: 915 Reward: -1513.7899 total_loss = 20.1907 = -0.3332 + 0.5 * 41.0314 + 0.01 * 0.8261\n",
      "-----------------\n",
      "Finished episode: 916 Reward: -1532.1081 total_loss = 18.5350 = -0.3048 + 0.5 * 37.6724 + 0.01 * 0.3567\n",
      "-----------------\n",
      "Finished episode: 917 Reward: -1573.2109 total_loss = 18.7699 = -0.3464 + 0.5 * 38.2186 + 0.01 * 0.7045\n",
      "-----------------\n",
      "Finished episode: 918 Reward: -1550.6415 total_loss = 17.5255 = -0.2841 + 0.5 * 35.5983 + 0.01 * 1.0482\n",
      "-----------------\n",
      "Finished episode: 919 Reward: -1551.0386 total_loss = 18.5846 = -0.2932 + 0.5 * 37.7353 + 0.01 * 1.0089\n",
      "-----------------\n",
      "Finished episode: 920 Reward: -1555.8937 total_loss = 19.2586 = -0.3528 + 0.5 * 39.2036 + 0.01 * 0.9579\n",
      "-----------------\n",
      "Finished episode: 921 Reward: -1567.2971 total_loss = 18.4986 = -0.3086 + 0.5 * 37.6089 + 0.01 * 0.2752\n",
      "-----------------\n",
      "Finished episode: 922 Reward: -1566.2981 total_loss = 17.8568 = -0.3272 + 0.5 * 36.3497 + 0.01 * 0.9156\n",
      "-----------------\n",
      "Finished episode: 923 Reward: -1525.0531 total_loss = 20.4991 = -0.2994 + 0.5 * 41.5846 + 0.01 * 0.6191\n",
      "-----------------\n",
      "Finished episode: 924 Reward: -1546.1558 total_loss = 17.6286 = -0.3511 + 0.5 * 35.9542 + 0.01 * 0.2648\n",
      "-----------------\n",
      "Finished episode: 925 Reward: -1582.4713 total_loss = 19.6415 = -0.3498 + 0.5 * 39.9730 + 0.01 * 0.4780\n",
      "-----------------\n",
      "Finished episode: 926 Reward: -1570.7714 total_loss = 20.2074 = -0.3017 + 0.5 * 41.0139 + 0.01 * 0.2243\n",
      "-----------------\n",
      "Finished episode: 927 Reward: -1572.6206 total_loss = 19.5419 = -0.3541 + 0.5 * 39.7820 + 0.01 * 0.5055\n",
      "-----------------\n",
      "Finished episode: 928 Reward: -1572.4001 total_loss = 18.9794 = -0.3349 + 0.5 * 38.6230 + 0.01 * 0.2766\n",
      "-----------------\n",
      "Finished episode: 929 Reward: -1575.7192 total_loss = 20.5961 = -0.3422 + 0.5 * 41.8599 + 0.01 * 0.8385\n",
      "-----------------\n",
      "Finished episode: 930 Reward: -1542.8531 total_loss = 19.5279 = -0.3716 + 0.5 * 39.7773 + 0.01 * 1.0780\n",
      "-----------------\n",
      "Finished episode: 931 Reward: -1571.4378 total_loss = 19.7220 = -0.3170 + 0.5 * 40.0780 + 0.01 * -0.0001\n",
      "-----------------\n",
      "Finished episode: 932 Reward: -1561.6860 total_loss = 18.6907 = -0.3290 + 0.5 * 38.0278 + 0.01 * 0.5769\n",
      "-----------------\n",
      "Finished episode: 933 Reward: -1560.0277 total_loss = 20.9136 = -0.2630 + 0.5 * 42.3408 + 0.01 * 0.6133\n",
      "-----------------\n",
      "Finished episode: 934 Reward: -1542.0997 total_loss = 18.4678 = -0.6568 + 0.5 * 38.2390 + 0.01 * 0.5055\n",
      "-----------------\n",
      "Finished episode: 935 Reward: -1561.6174 total_loss = 18.2495 = -0.3135 + 0.5 * 37.1260 + 0.01 * -0.0002\n",
      "-----------------\n",
      "Finished episode: 936 Reward: -1558.8010 total_loss = 20.2541 = -0.3532 + 0.5 * 41.1948 + 0.01 * 0.9892\n",
      "-----------------\n",
      "Finished episode: 937 Reward: -1545.8680 total_loss = 18.6917 = -0.3233 + 0.5 * 38.0232 + 0.01 * 0.3508\n",
      "-----------------\n",
      "Finished episode: 938 Reward: -1550.3380 total_loss = 18.8099 = -0.3514 + 0.5 * 38.2980 + 0.01 * 1.2296\n",
      "-----------------\n",
      "Finished episode: 939 Reward: -1554.7721 total_loss = 19.0188 = -0.3617 + 0.5 * 38.7449 + 0.01 * 0.8024\n",
      "-----------------\n",
      "Finished episode: 940 Reward: -1550.8606 total_loss = 19.1128 = -0.3174 + 0.5 * 38.8511 + 0.01 * 0.4621\n",
      "-----------------\n",
      "Finished episode: 941 Reward: -1565.2951 total_loss = 19.2154 = -0.3565 + 0.5 * 39.1269 + 0.01 * 0.8522\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode: 942 Reward: -1575.7748 total_loss = 19.5548 = -0.3279 + 0.5 * 39.7537 + 0.01 * 0.5835\n",
      "-----------------\n",
      "Finished episode: 943 Reward: -1567.5248 total_loss = 20.5300 = -1.1095 + 0.5 * 43.2650 + 0.01 * 0.6956\n",
      "-----------------\n",
      "Finished episode: 944 Reward: -1608.8994 total_loss = 21.5253 = -0.3123 + 0.5 * 43.6723 + 0.01 * 0.1417\n",
      "-----------------\n",
      "Finished episode: 945 Reward: -1560.0426 total_loss = 18.4493 = -0.2914 + 0.5 * 37.4647 + 0.01 * 0.8333\n",
      "-----------------\n",
      "Finished episode: 946 Reward: -1564.5237 total_loss = 18.8133 = -0.3462 + 0.5 * 38.2950 + 0.01 * 1.2015\n",
      "-----------------\n",
      "Finished episode: 947 Reward: -1574.3317 total_loss = 19.4804 = -0.3011 + 0.5 * 39.5336 + 0.01 * 1.4687\n",
      "-----------------\n",
      "Finished episode: 948 Reward: -1545.3433 total_loss = 19.1735 = -0.3360 + 0.5 * 38.9977 + 0.01 * 1.0626\n",
      "-----------------\n",
      "Finished episode: 949 Reward: -1591.1893 total_loss = 19.4005 = -0.3888 + 0.5 * 39.5593 + 0.01 * 0.9567\n",
      "-----------------\n",
      "Finished episode: 950 Reward: -1564.6810 total_loss = 18.7749 = -0.3173 + 0.5 * 38.1740 + 0.01 * 0.5235\n",
      "-----------------\n",
      "Finished episode: 951 Reward: -1566.1922 total_loss = 17.7195 = -0.3075 + 0.5 * 36.0411 + 0.01 * 0.6435\n",
      "-----------------\n",
      "Finished episode: 952 Reward: -1559.6227 total_loss = 22.7650 = -0.3043 + 0.5 * 46.1296 + 0.01 * 0.4583\n",
      "-----------------\n",
      "Finished episode: 953 Reward: -1592.2063 total_loss = 19.3640 = -0.3488 + 0.5 * 39.4127 + 0.01 * 0.6478\n",
      "-----------------\n",
      "Finished episode: 954 Reward: -1550.8778 total_loss = 18.2825 = -0.3178 + 0.5 * 37.1974 + 0.01 * 0.1543\n",
      "-----------------\n",
      "Finished episode: 955 Reward: -1556.8479 total_loss = 18.1249 = -0.7737 + 0.5 * 37.7761 + 0.01 * 1.0465\n",
      "-----------------\n",
      "Finished episode: 956 Reward: -1587.8029 total_loss = 18.1676 = -0.3145 + 0.5 * 36.9606 + 0.01 * 0.1779\n",
      "-----------------\n",
      "Finished episode: 957 Reward: -1549.1029 total_loss = 19.8836 = -0.2728 + 0.5 * 40.3014 + 0.01 * 0.5780\n",
      "-----------------\n",
      "Finished episode: 958 Reward: -1561.1916 total_loss = 20.0478 = -0.3129 + 0.5 * 40.7069 + 0.01 * 0.7286\n",
      "-----------------\n",
      "Finished episode: 959 Reward: -1549.4916 total_loss = 18.8012 = -0.3237 + 0.5 * 38.2366 + 0.01 * 0.6549\n",
      "-----------------\n",
      "Finished episode: 960 Reward: -1581.1171 total_loss = 20.0501 = -0.4324 + 0.5 * 40.9505 + 0.01 * 0.7195\n",
      "-----------------\n",
      "Finished episode: 961 Reward: -1562.5863 total_loss = 19.0669 = -0.3354 + 0.5 * 38.7968 + 0.01 * 0.3923\n",
      "-----------------\n",
      "Finished episode: 962 Reward: -1576.1725 total_loss = 19.3764 = -0.3472 + 0.5 * 39.4307 + 0.01 * 0.8182\n",
      "-----------------\n",
      "Finished episode: 963 Reward: -1571.4477 total_loss = 19.6797 = -0.3451 + 0.5 * 40.0304 + 0.01 * 0.9579\n",
      "-----------------\n",
      "Finished episode: 964 Reward: -1570.7840 total_loss = 20.1125 = -0.3753 + 0.5 * 40.9414 + 0.01 * 1.7112\n",
      "-----------------\n",
      "Finished episode: 965 Reward: -1576.3166 total_loss = 21.0224 = -0.3419 + 0.5 * 42.7163 + 0.01 * 0.6104\n",
      "-----------------\n",
      "Finished episode: 966 Reward: -1568.0044 total_loss = 16.4074 = -0.3227 + 0.5 * 33.4317 + 0.01 * 1.4207\n",
      "-----------------\n",
      "Finished episode: 967 Reward: -1570.0820 total_loss = 20.2361 = -0.5687 + 0.5 * 41.5799 + 0.01 * 1.4805\n",
      "-----------------\n",
      "Finished episode: 968 Reward: -1578.3596 total_loss = 20.1700 = -0.2994 + 0.5 * 40.9269 + 0.01 * 0.5897\n",
      "-----------------\n",
      "Finished episode: 969 Reward: -1558.6060 total_loss = 18.7711 = -0.3467 + 0.5 * 38.2300 + 0.01 * 0.2733\n",
      "-----------------\n",
      "Finished episode: 970 Reward: -1558.6743 total_loss = 18.8368 = -1.0615 + 0.5 * 39.7798 + 0.01 * 0.8450\n",
      "-----------------\n",
      "Finished episode: 971 Reward: -1563.2899 total_loss = 19.6368 = -0.3422 + 0.5 * 39.9135 + 0.01 * 2.2200\n",
      "-----------------\n",
      "Finished episode: 972 Reward: -1550.6172 total_loss = 20.2847 = -0.3676 + 0.5 * 41.2670 + 0.01 * 1.8828\n",
      "-----------------\n",
      "Finished episode: 973 Reward: -1556.7701 total_loss = 18.0555 = -0.3671 + 0.5 * 36.7984 + 0.01 * 2.3343\n",
      "-----------------\n",
      "Finished episode: 974 Reward: -1532.5499 total_loss = 18.2038 = -0.3194 + 0.5 * 37.0372 + 0.01 * 0.4619\n",
      "-----------------\n",
      "Finished episode: 975 Reward: -1562.7803 total_loss = 17.1829 = -0.2942 + 0.5 * 34.9385 + 0.01 * 0.7820\n",
      "-----------------\n",
      "Finished episode: 976 Reward: -1563.7249 total_loss = 17.9038 = -0.3104 + 0.5 * 36.4159 + 0.01 * 0.6283\n",
      "-----------------\n",
      "Finished episode: 977 Reward: -1567.7955 total_loss = 20.7794 = -0.3487 + 0.5 * 42.2313 + 0.01 * 1.2395\n",
      "-----------------\n",
      "Finished episode: 978 Reward: -1550.4847 total_loss = 19.2305 = -0.3114 + 0.5 * 39.0765 + 0.01 * 0.3648\n",
      "-----------------\n",
      "Finished episode: 979 Reward: -1577.4112 total_loss = 19.8736 = -0.2912 + 0.5 * 40.2992 + 0.01 * 1.5203\n",
      "-----------------\n",
      "Finished episode: 980 Reward: -1569.7561 total_loss = 20.6090 = -0.3687 + 0.5 * 41.9032 + 0.01 * 2.6145\n",
      "-----------------\n",
      "Finished episode: 981 Reward: -1576.5488 total_loss = 20.8771 = -0.3541 + 0.5 * 42.4244 + 0.01 * 1.8991\n",
      "-----------------\n",
      "Finished episode: 982 Reward: -1566.0627 total_loss = 21.3470 = -0.3244 + 0.5 * 43.3260 + 0.01 * 0.8410\n",
      "-----------------\n",
      "Finished episode: 983 Reward: -1575.1789 total_loss = 20.2298 = -0.3272 + 0.5 * 41.1108 + 0.01 * 0.1592\n",
      "-----------------\n",
      "Finished episode: 984 Reward: -1553.1629 total_loss = 18.4028 = -0.3715 + 0.5 * 37.5292 + 0.01 * 0.9726\n",
      "-----------------\n",
      "Finished episode: 985 Reward: -1561.3242 total_loss = 18.9829 = -0.3946 + 0.5 * 38.7276 + 0.01 * 1.3728\n",
      "-----------------\n",
      "Finished episode: 986 Reward: -1578.9012 total_loss = 20.3361 = -0.2759 + 0.5 * 41.2171 + 0.01 * 0.3496\n",
      "-----------------\n",
      "Finished episode: 987 Reward: -1564.7698 total_loss = 17.8148 = -0.2996 + 0.5 * 36.2043 + 0.01 * 1.2337\n",
      "-----------------\n",
      "Finished episode: 988 Reward: -1541.2860 total_loss = 20.7475 = -0.3263 + 0.5 * 42.1131 + 0.01 * 1.7246\n",
      "-----------------\n",
      "Finished episode: 989 Reward: -1544.9634 total_loss = 18.4519 = -0.3328 + 0.5 * 37.5433 + 0.01 * 1.3056\n",
      "-----------------\n",
      "Finished episode: 990 Reward: -1561.4201 total_loss = 19.0973 = -0.3275 + 0.5 * 38.8339 + 0.01 * 0.7884\n",
      "-----------------\n",
      "Finished episode: 991 Reward: -1566.0814 total_loss = 17.4073 = -0.3157 + 0.5 * 35.3923 + 0.01 * 2.6845\n",
      "-----------------\n",
      "Finished episode: 992 Reward: -1569.0119 total_loss = 19.3309 = -0.2952 + 0.5 * 39.1825 + 0.01 * 3.4807\n",
      "-----------------\n",
      "Finished episode: 993 Reward: -1568.1061 total_loss = 20.1792 = -0.3684 + 0.5 * 41.0553 + 0.01 * 2.0031\n",
      "-----------------\n",
      "Finished episode: 994 Reward: -1558.0715 total_loss = 19.3798 = -0.3106 + 0.5 * 39.2980 + 0.01 * 4.1417\n",
      "-----------------\n",
      "Finished episode: 995 Reward: -1571.3537 total_loss = 20.5004 = -0.2804 + 0.5 * 41.4175 + 0.01 * 7.2136\n",
      "-----------------\n",
      "Finished episode: 996 Reward: -1550.2592 total_loss = 17.9922 = -0.4823 + 0.5 * 36.8772 + 0.01 * 3.5890\n",
      "-----------------\n",
      "Finished episode: 997 Reward: -1557.4282 total_loss = 18.9876 = -0.5666 + 0.5 * 38.9212 + 0.01 * 9.3529\n",
      "-----------------\n",
      "Finished episode: 998 Reward: -1575.0348 total_loss = 18.3448 = -0.2901 + 0.5 * 37.0999 + 0.01 * 8.4961\n",
      "-----------------\n",
      "Finished episode: 999 Reward: -1540.2959 total_loss = 17.8494 = -0.3115 + 0.5 * 36.0528 + 0.01 * 13.4436\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# Here, you need to finish the first 5 tasks.\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, layer_norm=True):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        '''\n",
    "        Q1:\n",
    "        Initialize your networks\n",
    "        '''\n",
    "        self.actor_fc1 = nn.Linear(num_inputs,400)\n",
    "        self.actor_fc2 = nn.Linear(400,300)\n",
    "        self.actor_fc3 = nn.Linear(300,num_outputs)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, num_outputs))#作为参数参与优化{{0,0,0,0......}}\n",
    "\n",
    "        self.critic_fc1 = nn.Linear(num_inputs,400)\n",
    "        self.critic_fc2 = nn.Linear(400,300)\n",
    "        self.critic_fc3 = nn.Linear(300,num_outputs)\n",
    "\n",
    "        if layer_norm:\n",
    "            self.layer_norm(self.actor_fc1, std=1.0)\n",
    "            self.layer_norm(self.actor_fc2, std=1.0)\n",
    "            self.layer_norm(self.actor_fc3, std=0.01)\n",
    "\n",
    "            self.layer_norm(self.critic_fc1, std=1.0)\n",
    "            self.layer_norm(self.critic_fc2, std=1.0)\n",
    "            self.layer_norm(self.critic_fc3, std=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def layer_norm(layer, std=1.0, bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)#参数初始化-正交矩阵orthogonal matrix\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)#固定值fixed is bias_count\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        Q2.1:\n",
    "        run policy network (actor) as well as value network (critic)\n",
    "        :param states: a tensor represents states\n",
    "        :return: 3 Tensor2\n",
    "        your _forward_actor() function should return both the mean value of action and the log-standard deviation of the action\n",
    "        \"\"\"\n",
    "\n",
    "        action_mean, action_logstd = self._forward_actor(states)\n",
    "        critic_value = self._forward_critic(states)\n",
    "        return action_mean, action_logstd, critic_value\n",
    "\n",
    "    def _forward_actor(self, states):\n",
    "        '''\n",
    "        Q2.2:\n",
    "        build something like \n",
    "        x = activation (actor_fc(state))\n",
    "        the logstd output has already been provided\n",
    "        '''\n",
    "        \n",
    "        action_mean = self.actor_fc3(F.relu(self.actor_fc2(F.relu(self.actor_fc1(states)))))\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)#size of actor_logstd will be same with action_mean\n",
    "        return action_mean, action_logstd\n",
    "\n",
    "    def _forward_critic(self, states):\n",
    "        '''\n",
    "        Q2.3:\n",
    "        build something like \n",
    "        x = activation (critic_fc(state))'''\n",
    "\n",
    "        critic_value = self.critic_fc3(F.relu(self.critic_fc2(F.relu(self.critic_fc1(states)))))\n",
    "        return critic_value\n",
    "\n",
    "    def select_action(self, action_mean, action_logstd, return_logproba=True):\n",
    "        \"\"\"\n",
    "        Q3.1:\n",
    "        given mean and std, sample an action from normal(mean, std)\n",
    "        also returns probability of the given chosen\n",
    "        \"\"\"\n",
    "        action_std=torch.exp(action_logstd)\n",
    "        normal_distribution=torch.distributions.normal.Normal(action_mean,action_std)\n",
    "        action=normal_distribution.sample()\n",
    "        logproba=normal_distribution.log_prob(action)\n",
    "        return action, logproba\n",
    "\n",
    "    @staticmethod\n",
    "    def _normal_logproba(x, mean, logstd, std=None):\n",
    "        '''\n",
    "        Q3.2:\n",
    "        given a mean and logstd of a gaussian,\n",
    "        calculate the log-probability of a given x'''\n",
    "        std=torch.exp(logstd)\n",
    "        normal_distribution=torch.distributions.normal.Normal(mean,std)\n",
    "        logproba=normal_distribution.log_prob(x)\n",
    "        return logproba.sum(1)#对行求和\n",
    "\n",
    "    def get_logproba(self, states, actions):\n",
    "        \"\"\"\n",
    "        return probability of chosen the given actions under corresponding states of current network\n",
    "        :param states: Tensor\n",
    "        :param actions: Tensor\n",
    "        \"\"\"\n",
    "        action_mean, action_logstd = self._forward_actor(states)\n",
    "        action_mean = action_mean.cpu()#为什么要换成cpu\n",
    "        action_logstd = action_logstd.cpu()\n",
    "        logproba = self._normal_logproba(actions, action_mean, action_logstd)\n",
    "        return logproba\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return Transition(*zip(*self.memory))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "env = gym.make(ENV_NAME)  \n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "network.train()\n",
    "def ppo(args):\n",
    "    env = gym.make(args.env_name)\n",
    "    num_inputs = env.observation_space.shape[0]#3\n",
    "    num_actions = env.action_space.shape[0]#1\n",
    "\n",
    "    env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    #network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "    optimizer = opt.Adam(network.parameters(), lr=args.lr)\n",
    "\n",
    "    running_state = ZFilter((num_inputs,), clip=5.0)\n",
    "\n",
    "    # record average 1-round cumulative reward in every episode\n",
    "    reward_record = []\n",
    "    global_steps = 0 \n",
    "\n",
    "    lr_now = args.lr\n",
    "    clip_now = args.clip\n",
    "\n",
    "    for i_episode in range(args.num_episode):#num_episode=1000\n",
    "        # step1: perform current policy to collect trajectories\n",
    "        # this is an on-policy method!\n",
    "        memory = Memory()\n",
    "        num_steps = 0\n",
    "        reward_list = []\n",
    "        len_list = []\n",
    "        while num_steps < args.batch_size:#batch_size=5120\n",
    "            state = env.reset()#narray [a,c,b]\n",
    "            #????\n",
    "            if args.state_norm:#false\n",
    "                state = running_state(state)\n",
    "            reward_sum = 0\n",
    "            for t in range(args.max_step_per_round):#2000\n",
    "                action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))#unsqueeze add dimension=[[a,b,c]]\n",
    "                #all tensor action_mean[[a]],action_logstd[[a]],value[[a]]\n",
    "                action, logproba = network.select_action(action_mean, action_logstd)\n",
    "                #all tensor action[[a]],logproba[[a]]\n",
    "                action = action.cpu().data.numpy()[0]\n",
    "                logproba = logproba.cpu().data.numpy()[0]\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                reward_sum += reward\n",
    "                if args.state_norm:\n",
    "                    next_state = running_state(next_state)\n",
    "                mask = 0 if done else 1\n",
    "\n",
    "                memory.push(state, value, action, logproba, mask, next_state, reward)\n",
    "                #state=narrray[a.b.c], value=tensor[a], action=narray[a], logproba=narray[a], mask=float, next_state=state, reward=float\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            num_steps += (t + 1)\n",
    "            global_steps += (t + 1)\n",
    "            reward_list.append(reward_sum)\n",
    "            len_list.append(t + 1)\n",
    "\n",
    "        reward_record.append({\n",
    "            'episode': i_episode, \n",
    "            'steps': global_steps, \n",
    "            'meanepreward': np.mean(reward_list), \n",
    "            'meaneplen': np.mean(len_list)})\n",
    "        rwds.extend(reward_list)\n",
    "        batch = memory.sample()\n",
    "        batch_size = len(memory)#5200\n",
    "\n",
    "        # step2: extract variables from trajectories\n",
    "        rewards = Tensor(batch.reward)\n",
    "        #rewards=tensor[a,b,c....5200个]\n",
    "        values = Tensor(batch.value)\n",
    "        #values=[5200]\n",
    "        masks = Tensor(batch.mask)\n",
    "        #masks=[5200]\n",
    "        actions = Tensor(batch.action)\n",
    "        #actions=[5200,1]\n",
    "        states = Tensor(batch.state)\n",
    "        #states=[5200,3]\n",
    "        oldlogproba = Tensor(batch.logproba)\n",
    "        #oldlogproba=[5200,1]\n",
    "\n",
    "        #print(rewards)\n",
    "        returns = Tensor(batch_size)\n",
    "        deltas = Tensor(batch_size)\n",
    "        advantages = Tensor(batch_size)\n",
    "\n",
    "        prev_return = 0\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "        for i in reversed(range(batch_size)):#5200-0\n",
    "            returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n",
    "            deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values[i]\n",
    "            # ref: https://arxiv.org/pdf/1506.02438.pdf (generalization advantage estimate)\n",
    "            advantages[i] = deltas[i] + args.gamma * args.lamda * prev_advantage * masks[i]\n",
    "\n",
    "            prev_return = returns[i]\n",
    "            prev_value = values[i]\n",
    "            prev_advantage = advantages[i]\n",
    "        #returns.size()=[5200],deltas.size()=[5200],advantages.size()=[5200]\n",
    "        if args.advantage_norm:#true\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + EPS)\n",
    "\n",
    "        for i_epoch in range(int(args.num_epoch * batch_size / args.minibatch_size)):#10*5200/256=200\n",
    "            # sample from current batch\n",
    "            minibatch_ind = np.random.choice(batch_size, args.minibatch_size, replace=False)#从0-5200选256个数\n",
    "            #minibatch_ind=narray[256]\n",
    "            minibatch_states = states[minibatch_ind]\n",
    "            #minibatch_states.size()=tensor[256,3]\n",
    "            minibatch_actions = actions[minibatch_ind]\n",
    "            #minibatch_actions.size()=tensor[256,1]\n",
    "            minibatch_oldlogproba = oldlogproba[minibatch_ind]\n",
    "            #minibatch_oldlogproba.size()=[256.1]\n",
    "            minibatch_newlogproba = network.get_logproba(minibatch_states, minibatch_actions)\n",
    "            #minibatch_newlogproba.size()=tensor[256]\n",
    "            minibatch_advantages = advantages[minibatch_ind]\n",
    "            #minibatch_advantages.size()=tensor[256]\n",
    "            minibatch_returns = returns[minibatch_ind]\n",
    "            #minibatch_returns.size()=tensor[256]\n",
    "            minibatch_newvalues = network._forward_critic(minibatch_states).flatten()\n",
    "            #minibatch_newvalues.size()=tensor[256]\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            Q4: \n",
    "\n",
    "            HERE: \n",
    "            now you have the advantages, and log-probabilities (both pi_new and pi_old)\n",
    "            you need to do optimization according to the CLIP loss\n",
    "            \n",
    "            '''\n",
    "            minibatch_loss_surr=Tensor(args.minibatch_size)\n",
    "            #minibatch_loss_surr.size()=tensor[256]\n",
    "            #print(minibatch_oldlogproba.view(-1).size())\n",
    "            ratio=torch.exp(minibatch_newlogproba)/torch.exp(minibatch_oldlogproba.view(-1))\n",
    "            for i in range(args.minibatch_size):\n",
    "                if minibatch_advantages[i] >0:\n",
    "                    minibatch_loss_surr[i] = min(ratio[i],1+args.clip)*minibatch_advantages[i]\n",
    "                else:\n",
    "                    minibatch_loss_surr[i] = max(ratio[i],1-args.clip)*minibatch_advantages[i]\n",
    "            loss_surr=torch.mean(minibatch_loss_surr)\n",
    "            #print(loss_surr) tensor a 0维\n",
    "           \n",
    "            if args.lossvalue_norm:\n",
    "                minibatch_return_6std = 6 * minibatch_returns.std()\n",
    "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2)) / minibatch_return_6std\n",
    "            else:\n",
    "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2))\n",
    "\n",
    "            loss_entropy = torch.mean(torch.exp(minibatch_newlogproba) * minibatch_newlogproba)\n",
    "            #print(loss_value,loss_entropy) 都是0维\n",
    "            \n",
    "            total_loss = loss_surr + args.loss_coeff_value * loss_value + args.loss_coeff_entropy * loss_entropy\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.schedule_clip == 'linear':\n",
    "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "            clip_now = args.clip * ep_ratio\n",
    "\n",
    "        if args.schedule_adam == 'linear':\n",
    "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "            lr_now = args.lr * ep_ratio\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr_now\n",
    "\n",
    "        if i_episode % args.log_num_episode == 0:\n",
    "            print('Finished episode: {} Reward: {:.4f} total_loss = {:.4f} = {:.4f} + {} * {:.4f} + {} * {:.4f}' \\\n",
    "                .format(i_episode, reward_record[-1]['meanepreward'], total_loss.data, loss_surr.data, args.loss_coeff_value, \n",
    "                loss_value.data, args.loss_coeff_entropy, loss_entropy.data))\n",
    "            print('-----------------')\n",
    "\n",
    "    return reward_record\n",
    "\n",
    "def test(args):\n",
    "    record_dfs = []\n",
    "    for i in range(args.num_parallel_run):\n",
    "        args.seed += 1\n",
    "        reward_record = pd.DataFrame(ppo(args))\n",
    "        reward_record['#parallel_run'] = i\n",
    "        record_dfs.append(reward_record)\n",
    "    record_dfs = pd.concat(record_dfs, axis=0)\n",
    "    record_dfs.to_csv(joindir(RESULT_DIR, 'ppo-record-{}.csv'.format(args.env_name)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for envname in [ENV_NAME]:\n",
    "        args.env_name = envname\n",
    "        test(args)\n",
    "\n",
    "torch.save(network.state_dict(),args.env_name.split('-')[0]+'/CheckPoints/checkpoint_new_{0}hidden_{1}drop_prob'.format(args.hid_num,args.drop_prob)) \n",
    "np.savetxt(args.env_name.split('-')[0]+'/Rwds/rwds_new_{0}hidden_{1}drop_prob'.format(args.hid_num,args.drop_prob),rwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVV1-fJWArMt"
   },
   "source": [
    "# DDPG and TD3\n",
    "\n",
    "The Deterministic Policy Gradient method was proposed by Silver et. al. 2014 (http://proceedings.mlr.press/v32/silver14.pdf), and DDPG is its deep version.\n",
    "\n",
    "The DPG also uses the actor-critic paradigm, but maitains a deterministic version of policy. It optimizes the critic through the Bellman Equation, and optimize the actor through the chain rule. \n",
    "\n",
    "In this assignment, you may need to import some python files like DDPG.py and TD3.py to insert the method into training.\n",
    "Here are some solutions from stackoverflow: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab.\n",
    "\n",
    "It is easier to just copy it from Drive than upload it.\n",
    "1. Store MYLIB.py in your Drive. (for this assignment, it will be the utils.py, DDPG.py and TD3.py)\n",
    "2. Open the Colab.\n",
    "3. Open the left side pane, select Files view (the file icon).\n",
    "4. Click Mount Drive then Connect to Google Drive (the folder with google drive icon).\n",
    "5. Copy it by running \"! cp drive/My\\ Drive/MYLIB.py . \" in your Colab file code line.\n",
    "6. import MYLIB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6OJdsG_MJz_"
   },
   "source": [
    "## TODOs for You (Please write down the answer in this block)\n",
    "\n",
    "The TD3 is short for *Twin Delayed Deep Deterministic Policy Gradient*, their official open-source implementation is extremely clear and easy to follow! So I believe there is no need for you to build up the wheels one more time.\n",
    "\n",
    "However, you really need to know about how this method works!\n",
    "TD3 proposes several improvements based on the method of DDPG to improve its sample efficiency.\n",
    "\n",
    "- Q6. In this part, your task is to read the paper, and read the code of the official implementation of TD3 and DDPG at:\n",
    "\n",
    "https://github.com/sfujim/TD3/blob/master/DDPG.py\n",
    "\n",
    "https://github.com/sfujim/TD3/blob/master/TD3.py\n",
    "\n",
    "Then, please try to find the proposed improvements in TD3 over DDPG and summary them HERE:\n",
    "\n",
    "1. TD3 learns two Q-functions instead of one(DDPG learns one).and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.\n",
    "    - code:\n",
    "         target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "         target_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\t target_Q = reward + not_done * self.discount * target_Q\n",
    "2. TD3 upadtes the policy less frequently than the Q-function.TD3 updates the policy after two updates of the Q-function(DDPG updates the policy for every two Q-function updates).\n",
    "    - code:\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\t\t\t# Compute actor losse\n",
    "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "3. TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.\n",
    "    - code:\n",
    "        noise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip) \n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "3. Actor update of TD3 only depends on part of the network gradient.\n",
    "     - code:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "     \n",
    "- Q7. Among all those improvements, which do you believe is the most important one? You may take some ablation studies to support your claim.  (i.e., draw some learning curves with different settings together and draw your conclusions).\n",
    "\n",
    " In my opinion,TD3 learns two Q-functions is the most important one.Because the main problem of DDPG is that the learned Q-function sometimes dramatically overestimate Q-values, which then leads to the policy breaking.TD3 uses learning two Q-functions and selecting smaller one to solve this problem.\n",
    " \n",
    " \n",
    "- Q8. What is the difference between TD3(DDPG) and PPO in the OPTIMIZATION step (including but not restricted in terms of the sampling-training proportion)? Actually the improvements of PPO over TRPO was pointed  as a benefit of more training iterations, can you further improve the sample efficiency of TD3?\n",
    " \n",
    " 1- PPO use importance sampling to exploit the data.In an training phase,firstly,PPO gets all transition and put them in replay buffer,then uses part of them to train and update network.The reason why exploit the same data is because it uses importance sampling.\n",
    "    TD3(DDPG) gets a transition then trains and update network,after trainning and upadating, it will get another new transition to execute next train.In a word,it will use different policy to get new data for train.\n",
    " 2- TD3 optimize actor and critic seperately.PPO optimize actor and critic using a loss at the same time.\n",
    " \n",
    " To improve the sample efficiency of TD3,I think we can imitate the sampling strategy of PPO:Fristly accumulating enough trasition,Then sample the mini-batch size transitions for next train.\n",
    " \n",
    "\n",
    "- Q9. (i) Please describe the difference of the exploration strategies between PPO, DDPG and TD3. (ii) Provide a comparison between the exploration strategies of those continuous control algorithms and DQN.\n",
    "\n",
    " (i)TD3：TD3 use the actor network to get the action mean and anction logstd,which represent a normal distribution,then according this distribution TD3 will select an action.\n",
    "   PPO(DDPG):Before starting trainning,PPO(DDPG) will randomly select an action between the max and the min and get a transition.After starting trainning,PPO(DDPG) will get the action through actor network with certain probability then constraint this action by clip,otherwise randomly select an action between the max and the min. \n",
    "  (ii)DQN uses an epsilon-greedy algorithm to select action.It will select an action which take the maxmum value with epsilon probability and select an action randomly with 1-epsilon probability.\n",
    "   The actions selected by DQN are discrete variables, while the actions TD3(DDPG) and PPO choose are continuous variables within a certain range. \n",
    "   \n",
    "   \n",
    "- Q10. (Bonus, 20 points) An open question. Do you think an epsilon-greedy-like exploration strategy you used in DQN/Q-learning is useful for continuous control? Will there be any problem of applying epsilon-greedy method in DDPG/TD3/PPO? Try to implement the idea and report the results.\n",
    "\n",
    " Yes.It is useful.I apply it with PPO,leading the higher final reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following four blocks download the code in official implementation to your google drive so that the following script can run them. Note that the downloaded files may disappear due to some colab mechansim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "R2jXxeaWoCPM",
    "outputId": "31d18c26-8186-49ba-bcfc-ee72300fed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TD3'...\n",
      "remote: Enumerating objects: 223, done.\u001b[K\n",
      "remote: Total 223 (delta 0), reused 0 (delta 0), pack-reused 223\u001b[K\n",
      "Receiving objects: 100% (223/223), 197.79 KiB | 691.00 KiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sfujim/TD3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "66Sy-dOQ8Qo5"
   },
   "outputs": [],
   "source": [
    "!cp TD3/DDPG.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5TuZqwcHottp"
   },
   "outputs": [],
   "source": [
    "!cp TD3/TD3.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "259dqcCpotrJ"
   },
   "outputs": [],
   "source": [
    "!cp TD3/utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2X3lvH_5pKkS"
   },
   "outputs": [],
   "source": [
    "from os import makedirs as mkdir\n",
    "mkdir('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lboiuk9vHDhD"
   },
   "outputs": [],
   "source": [
    "# The following scripts run the DDPG algorithm.\n",
    "\n",
    "alias = 'ddpg' # an alias of your experiment, used as a label\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3\n",
    "import DDPG\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done,_ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    #print(\"---------------------------------------\")\n",
    "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    #print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 100000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 25\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 0\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "\n",
    "args_policy = 'DDPG'\n",
    "\n",
    "if args_policy == \"TD3\":\n",
    "    # Target policy smoothing is scaled wrt the action scale\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3.TD3(**kwargs)\n",
    "elif args_policy == \"DDPG\":\n",
    "    policy = DDPG.DDPG(**kwargs)\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "evaluations = [eval_policy(policy)]\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "counter = 0\n",
    "msk_list = []        \n",
    "temp_curve = [eval_policy(policy)]\n",
    "temp_val = []\n",
    "for t in range(int(args_max_timesteps)):\n",
    "    episode_timesteps += 1\n",
    "    counter += 1\n",
    "    # Select action randomly or according to policy\n",
    "    if t < args_start_timesteps:\n",
    "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "    else:\n",
    "        if np.random.uniform(0,1) < 0.1:\n",
    "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "        else:\n",
    "            action = ( \n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, done,_ = env.step(action) \n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if t >= args_start_timesteps:\n",
    "        '''TD3'''\n",
    "        last_val = 999.\n",
    "        patient = 5\n",
    "        for i in range(1):\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                \n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if done: \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        msk_list = []\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args_eval_freq == 0:\n",
    "        evaluations.append(eval_policy(policy))\n",
    "        print('recent Evaluation:',evaluations[-1])\n",
    "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAVa6S3yG5m5"
   },
   "outputs": [],
   "source": [
    "# The following scripts run the TD3 algorithm.\n",
    "\n",
    "alias = 'td3'\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3\n",
    "import DDPG\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done,_ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    #print(\"---------------------------------------\")\n",
    "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    #print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 100000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 25\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 0\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "\n",
    "args_policy = 'TD3'\n",
    "\n",
    "if args_policy == \"TD3\":\n",
    "    # Target policy smoothing is scaled wrt the action scale\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3.TD3(**kwargs)\n",
    "elif args_policy == \"OurDDPG\":\n",
    "    policy = OurDDPG.DDPG(**kwargs)\n",
    "elif args_policy == \"DDPG\":\n",
    "    policy = DDPG.DDPG(**kwargs)\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "evaluations = [eval_policy(policy)]\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "counter = 0\n",
    "msk_list = []        \n",
    "temp_curve = [eval_policy(policy)]\n",
    "temp_val = []\n",
    "for t in range(int(args_max_timesteps)):\n",
    "    episode_timesteps += 1\n",
    "    counter += 1\n",
    "    # Select action randomly or according to policy\n",
    "    if t < args_start_timesteps:\n",
    "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "    else:\n",
    "        if np.random.uniform(0,1) < 0.1:\n",
    "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, done,_ = env.step(action)\n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if t >= args_start_timesteps:\n",
    "        '''TD3'''\n",
    "        last_val = 999.\n",
    "        patient = 5\n",
    "        for i in range(1):\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                \n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if done: \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        msk_list = []\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args_eval_freq == 0:\n",
    "        evaluations.append(eval_policy(policy))\n",
    "        print('recent Evaluation:',evaluations[-1])\n",
    "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KboX9hB5eEVv"
   },
   "source": [
    "# Four-Solution-Maze Environment (optional)\n",
    "\n",
    "## TODOs for you:\n",
    "\n",
    "- Q11. (bonus) In this section, another environment named Four-Solution-Maze is provided for you to evaluate your algorithms.\n",
    "\n",
    "The task is quite simple, yet never easy for even PPO/TD3.\n",
    "\n",
    "The default size of the maze is 64x64, and in each game (espisode), the agent is initialized randomly in the maze. There are 4 positions in the maze that has non-trivial reward of +10, while reaching other region will recieve only a tiny punishment of -0.1. An optimal policy should be able to find the shortest path to the most recent reward region (i.e., one of the four high-reward regions.).\n",
    "\n",
    "The action space is continuous with range [-1,1], larger actions will be clipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX6V-0bfMSev"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "import copy\n",
    "\n",
    "class FourWayGridWorld:\n",
    "    def __init__(self, N=17,left = 10,right = 10, up=10, down = 10):\n",
    "        self.N = N\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.map = np.ones((N,N))*(-0.1)\n",
    "        self.map[int((N-1)/2),0] = self.left\n",
    "        self.map[0,int((N-1)/2)] = self.up\n",
    "        self.map[N-1,int((N-1)/2)] = self.down\n",
    "        self.map[int((N-1)/2),N-1] = self.right\n",
    "        self.loc = np.asarray([np.random.randint(N),np.random.randint(N)])\n",
    "        self.step_num = 0\n",
    "    def step(self,action):\n",
    "        action = np.clip(action,-1,1)\n",
    "        new_loc = np.clip(self.loc + action,0,self.N-1)\n",
    "        self.loc = new_loc\n",
    "        reward = self.map[int(round(self.loc[0])),int(round(self.loc[1]))]\n",
    "        self.step_num+=1\n",
    "        return self.loc,reward,self.ifdone()\n",
    "    def ifdone(self):\n",
    "        if self.step_num >= 2*self.N:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def render(self):\n",
    "        map_self = copy.deepcopy(self.map)\n",
    "        map_self[int(self.loc[0]),int(self.loc[1])] = -5\n",
    "        plt.imshow(map_self)\n",
    "    def reset(self):\n",
    "        self.map = np.ones((self.N,self.N))*(-0.1)\n",
    "        self.map[int((self.N-1)/2),0] = self.left\n",
    "        self.map[0,int((self.N-1)/2)] = self.up\n",
    "        self.map[self.N-1,int((self.N-1)/2)] = self.down\n",
    "        self.map[int((self.N-1)/2),self.N-1] = self.right\n",
    "        self.loc = np.asarray([np.random.randint(self.N),np.random.randint(self.N)])\n",
    "        self.step_num = 0\n",
    "        return self.loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoO_Ti-zhykw"
   },
   "outputs": [],
   "source": [
    "env = FourWayGridWorld(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rBzb2qw3h2ts",
    "outputId": "458ee588-3f5b-4f19-d45c-952b6b330a73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30,  2])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "SBcwx2WPh3nT",
    "outputId": "1b60ea64-3859-4feb-8bbf-db6e72ca287e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WrHjDQmqC7K"
   },
   "outputs": [],
   "source": [
    "# This section is used to visualize your learned policy\n",
    "from torch import Tensor\n",
    "output_i = np.zeros((33,33))\n",
    "output_j = np.zeros((33,33))\n",
    "output_i_m = np.zeros((33,33))\n",
    "output_j_m = np.zeros((33,33))\n",
    "value_ij = np.zeros((33,33))\n",
    "for i in range(33):\n",
    "    for j in range(33):\n",
    "        states = Tensor(np.asarray([i,j])).float().unsqueeze(0)\n",
    "        \n",
    "        '''\n",
    "        you need to revise the following line, \n",
    "        to fit your policy/network outputs\n",
    "        '''\n",
    "        action, value = policy(states)\n",
    "        output_i[i,j] = action[0]\n",
    "        output_j[i,j] = action[1]\n",
    "        value_ij[i,j] = value\n",
    "        \n",
    "plt.figure(figsize= (5,5))\n",
    "for i in range(33):\n",
    "    for j in range(33):\n",
    "        plt.arrow(j,-i,output_j[i,j],-output_i[i,j],head_width=0.2,shape='left')\n",
    "xlim(-1,33)\n",
    "ylim(-33,1)\n",
    "yticks([2*i-32 for i in range(17)],[2*i for i in range(17)])\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5YEdvVdqDd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4_W/O_Sol.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
